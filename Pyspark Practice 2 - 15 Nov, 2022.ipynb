{"cells":[{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e25d6f5-d627-4591-a323-8ab7be76e9b1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3608311878953391#setting/sparkui/1115-110204-6b1hu6qa/driver-8953665437868112782\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3608311878953391#setting/sparkui/1115-110204-6b1hu6qa/driver-8953665437868112782\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a604ae5c-9067-439a-83d7-e2b92af3125f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark=SparkSession.builder.appName(\"Practice\").getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f059b1b3-1c40-4361-8ead-ebc16d2d7268","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(spark)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d7166cea-9c0b-4f8b-891a-e5140815e2d4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<pyspark.sql.session.SparkSession object at 0x7f9dfd224f10>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<pyspark.sql.session.SparkSession object at 0x7f9dfd224f10>\n"]}}],"execution_count":0},{"cell_type":"code","source":["#creating RDD\nmy_rdd=sc.parallelize([1,2,3,4,5,6,7,8,9])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d795047-d3e2-463f-bd56-aa1979c0d664","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(my_rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"783d8147-cdc4-46d9-b78f-82b35244948e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: pyspark.rdd.RDD","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: pyspark.rdd.RDD"]}}],"execution_count":0},{"cell_type":"code","source":["sparkContext=spark.sparkContext\nmy_rdd1=sparkContext.parallelize([1,2,3,4,5,6,7,8,9])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79a06435-f23b-47af-a958-ab270ec63674","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(my_rdd1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a82f5761-038a-4a4a-87bd-f48d0d54dbec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[12]: pyspark.rdd.RDD","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[12]: pyspark.rdd.RDD"]}}],"execution_count":0},{"cell_type":"code","source":["my_rdd.sum()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a23fdc7c-f3c2-4492-bab3-cb866cec79d3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[13]: 45","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[13]: 45"]}}],"execution_count":0},{"cell_type":"code","source":["spark=spark.sparkContext\nmy_rdd2=spark.parallelize([1,2,3,4,5,6,7,8,9])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60a790ec-c187-4b51-9e05-2f04eee6fa09","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["my_rdd.max()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2fbf3454-59e4-496c-adf0-2adfd97636f4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[16]: 9","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[16]: 9"]}}],"execution_count":0},{"cell_type":"code","source":["my_rdd.min()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de51c813-a25d-4ea7-8416-cd83750b213f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[17]: 1","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[17]: 1"]}}],"execution_count":0},{"cell_type":"code","source":["#distinct values in RDD\nx=sc.parallelize(['A','B','A','C','B','D','C'])\ny=x.distinct()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab63c96c-1f67-4649-88f4-14832ec85741","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"PythonRDD[29] at RDD at PythonRDD.scala:58\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["PythonRDD[29] at RDD at PythonRDD.scala:58\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(x)\nprint(y)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9b362e8c-34e0-4250-b91d-af72d5af00f7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"ParallelCollectionRDD[24] at readRDDFromInputStream at PythonRDD.scala:435\nPythonRDD[29] at RDD at PythonRDD.scala:58\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["ParallelCollectionRDD[24] at readRDDFromInputStream at PythonRDD.scala:435\nPythonRDD[29] at RDD at PythonRDD.scala:58\n"]}}],"execution_count":0},{"cell_type":"code","source":["print(x.collect())\nprint(y.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dc08b413-38ae-40eb-abb6-d33cca3c92e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['A', 'B', 'A', 'C', 'B', 'D', 'C']\n['B', 'C', 'A', 'D']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['A', 'B', 'A', 'C', 'B', 'D', 'C']\n['B', 'C', 'A', 'D']\n"]}}],"execution_count":0},{"cell_type":"code","source":["dir(x)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c444449-dce5-4fa5-b83c-dc7ff8c6240c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[27]: ['__add__',\n '__class__',\n '__class_getitem__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__orig_bases__',\n '__parameters__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__slots__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_computeFractionForSampleSize',\n '_defaultReducePartitions',\n '_id',\n '_is_barrier',\n '_is_protocol',\n '_jrdd',\n '_jrdd_deserializer',\n '_memory_limit',\n '_pickled',\n '_reserialize',\n '_to_java_object_rdd',\n 'aggregate',\n 'aggregateByKey',\n 'barrier',\n 'cache',\n 'cartesian',\n 'checkpoint',\n 'cleanShuffleDependencies',\n 'coalesce',\n 'cogroup',\n 'collect',\n 'collectAsMap',\n 'collectWithJobGroup',\n 'combineByKey',\n 'context',\n 'count',\n 'countApprox',\n 'countApproxDistinct',\n 'countByKey',\n 'countByValue',\n 'ctx',\n 'distinct',\n 'filter',\n 'first',\n 'flatMap',\n 'flatMapValues',\n 'fold',\n 'foldByKey',\n 'foreach',\n 'foreachPartition',\n 'fullOuterJoin',\n 'getCheckpointFile',\n 'getNumPartitions',\n 'getResourceProfile',\n 'getStorageLevel',\n 'glom',\n 'groupBy',\n 'groupByKey',\n 'groupWith',\n 'has_resource_profile',\n 'histogram',\n 'id',\n 'intersection',\n 'isCheckpointed',\n 'isEmpty',\n 'isLocallyCheckpointed',\n 'is_cached',\n 'is_checkpointed',\n 'join',\n 'keyBy',\n 'keys',\n 'leftOuterJoin',\n 'localCheckpoint',\n 'lookup',\n 'map',\n 'mapPartitions',\n 'mapPartitionsWithIndex',\n 'mapPartitionsWithSplit',\n 'mapValues',\n 'max',\n 'mean',\n 'meanApprox',\n 'min',\n 'name',\n 'partitionBy',\n 'partitioner',\n 'persist',\n 'pipe',\n 'randomSplit',\n 'reduce',\n 'reduceByKey',\n 'reduceByKeyLocally',\n 'repartition',\n 'repartitionAndSortWithinPartitions',\n 'rightOuterJoin',\n 'sample',\n 'sampleByKey',\n 'sampleStdev',\n 'sampleVariance',\n 'saveAsHadoopDataset',\n 'saveAsHadoopFile',\n 'saveAsNewAPIHadoopDataset',\n 'saveAsNewAPIHadoopFile',\n 'saveAsPickleFile',\n 'saveAsSequenceFile',\n 'saveAsTextFile',\n 'setName',\n 'sortBy',\n 'sortByKey',\n 'stats',\n 'stdev',\n 'subtract',\n 'subtractByKey',\n 'sum',\n 'sumApprox',\n 'take',\n 'takeOrdered',\n 'takeSample',\n 'toDF',\n 'toDebugString',\n 'toLocalIterator',\n 'top',\n 'treeAggregate',\n 'treeReduce',\n 'union',\n 'unpersist',\n 'values',\n 'variance',\n 'withResources',\n 'zip',\n 'zipWithIndex',\n 'zipWithUniqueId']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[27]: ['__add__',\n '__class__',\n '__class_getitem__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__orig_bases__',\n '__parameters__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__slots__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_computeFractionForSampleSize',\n '_defaultReducePartitions',\n '_id',\n '_is_barrier',\n '_is_protocol',\n '_jrdd',\n '_jrdd_deserializer',\n '_memory_limit',\n '_pickled',\n '_reserialize',\n '_to_java_object_rdd',\n 'aggregate',\n 'aggregateByKey',\n 'barrier',\n 'cache',\n 'cartesian',\n 'checkpoint',\n 'cleanShuffleDependencies',\n 'coalesce',\n 'cogroup',\n 'collect',\n 'collectAsMap',\n 'collectWithJobGroup',\n 'combineByKey',\n 'context',\n 'count',\n 'countApprox',\n 'countApproxDistinct',\n 'countByKey',\n 'countByValue',\n 'ctx',\n 'distinct',\n 'filter',\n 'first',\n 'flatMap',\n 'flatMapValues',\n 'fold',\n 'foldByKey',\n 'foreach',\n 'foreachPartition',\n 'fullOuterJoin',\n 'getCheckpointFile',\n 'getNumPartitions',\n 'getResourceProfile',\n 'getStorageLevel',\n 'glom',\n 'groupBy',\n 'groupByKey',\n 'groupWith',\n 'has_resource_profile',\n 'histogram',\n 'id',\n 'intersection',\n 'isCheckpointed',\n 'isEmpty',\n 'isLocallyCheckpointed',\n 'is_cached',\n 'is_checkpointed',\n 'join',\n 'keyBy',\n 'keys',\n 'leftOuterJoin',\n 'localCheckpoint',\n 'lookup',\n 'map',\n 'mapPartitions',\n 'mapPartitionsWithIndex',\n 'mapPartitionsWithSplit',\n 'mapValues',\n 'max',\n 'mean',\n 'meanApprox',\n 'min',\n 'name',\n 'partitionBy',\n 'partitioner',\n 'persist',\n 'pipe',\n 'randomSplit',\n 'reduce',\n 'reduceByKey',\n 'reduceByKeyLocally',\n 'repartition',\n 'repartitionAndSortWithinPartitions',\n 'rightOuterJoin',\n 'sample',\n 'sampleByKey',\n 'sampleStdev',\n 'sampleVariance',\n 'saveAsHadoopDataset',\n 'saveAsHadoopFile',\n 'saveAsNewAPIHadoopDataset',\n 'saveAsNewAPIHadoopFile',\n 'saveAsPickleFile',\n 'saveAsSequenceFile',\n 'saveAsTextFile',\n 'setName',\n 'sortBy',\n 'sortByKey',\n 'stats',\n 'stdev',\n 'subtract',\n 'subtractByKey',\n 'sum',\n 'sumApprox',\n 'take',\n 'takeOrdered',\n 'takeSample',\n 'toDF',\n 'toDebugString',\n 'toLocalIterator',\n 'top',\n 'treeAggregate',\n 'treeReduce',\n 'union',\n 'unpersist',\n 'values',\n 'variance',\n 'withResources',\n 'zip',\n 'zipWithIndex',\n 'zipWithUniqueId']"]}}],"execution_count":0},{"cell_type":"code","source":["help(x)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a32b3a9-a5aa-492a-81ec-0c670bec2f69","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Help on RDD in module pyspark.rdd object:\n\nclass RDD(typing.Generic)\n |  RDD(jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n |  \n |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n |  Represents an immutable, partitioned collection of elements that can be\n |  operated on in parallel.\n |  \n |  Method resolution order:\n |      RDD\n |      typing.Generic\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n |      Return the union of this RDD and another one.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n |      >>> (rdd + rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  __getnewargs__(self) -> NoReturn\n |  \n |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given combine functions and a neutral \"zero\n |      value.\"\n |      \n |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      The first function (seqOp) can return a different result type, U, than\n |      the type of this RDD. Thus, we need one operation for merging a T into\n |      an U and one operation for merging two U\n |      \n |      Examples\n |      --------\n |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n |      (10, 4)\n |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n |      (0, 0)\n |  \n |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, U]]'\n |      Aggregate the values of each key, using given combine functions and a neutral\n |      \"zero value\". This function can return a different result type, U, than the type\n |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n |      a U and one operation for merging two U's, The former operation is used for merging\n |      values within a partition, and the latter is used for merging values between\n |      partitions. To avoid memory allocation, both of these functions are\n |      allowed to modify and return their first argument instead of creating a new U.\n |  \n |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n |      entire stage and relaunch all tasks for this stage.\n |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Returns\n |      -------\n |      :class:`RDDBarrier`\n |          instance that provides actions within a barrier stage.\n |      \n |      See Also\n |      --------\n |      pyspark.BarrierTaskContext\n |      \n |      Notes\n |      -----\n |      For additional information see\n |      \n |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n |      \n |      This API is experimental\n |  \n |  cache(self: 'RDD[T]') -> 'RDD[T]'\n |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n |  \n |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n |      Return the Cartesian product of this RDD and another one, that is, the\n |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n |      ``b`` is in `other`.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2])\n |      >>> sorted(rdd.cartesian(rdd).collect())\n |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n |  \n |  checkpoint(self) -> None\n |      Mark this RDD for checkpointing. It will be saved to a file inside the\n |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n |      all references to its parent RDDs will be removed. This function must\n |      be called before any job has been executed on this RDD. It is strongly\n |      recommended that this RDD is persisted in memory, otherwise saving it\n |      on a file will require recomputation.\n |  \n |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n |      Removes an RDD's shuffles and it's non-persisted ancestors.\n |      \n |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n |      If you use the RDD after this call, you should checkpoint and materialize it first.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      Parameters\n |      ----------\n |      blocking : bool, optional\n |         block on shuffle cleanup tasks. Disabled by default.\n |      \n |      Notes\n |      -----\n |      This API is a developer API.\n |  \n |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n |      Return a new RDD that is reduced into `numPartitions` partitions.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n |      [[1], [2, 3], [4, 5]]\n |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n |      [[1, 2, 3, 4, 5]]\n |  \n |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n |      For each key k in `self` or `other`, return a resulting RDD that\n |      contains a tuple with the list of values for that key in `self` as\n |      well as `other`.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> y = sc.parallelize([(\"a\", 2)])\n |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n |      [('a', ([1], [2])), ('b', ([4], []))]\n |  \n |  collect(self: 'RDD[T]') -> List[~T]\n |      Return a list that contains all of the elements in this RDD.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |  \n |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n |      Return the key-value pairs in this RDD to the master as a dictionary.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting data is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n |      >>> m[1]\n |      2\n |      >>> m[3]\n |      4\n |  \n |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n |      When collect rdd, use this method to specify job group.\n |      \n |      .. versionadded:: 3.0.0\n |      .. deprecated:: 3.1.0\n |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n |  \n |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, U]]'\n |      Generic function to combine the elements for each key using a custom\n |      set of aggregation functions.\n |      \n |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n |      type\" C.\n |      \n |      Users provide three functions:\n |      \n |          - `createCombiner`, which turns a V into a C (e.g., creates\n |            a one-element list)\n |          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n |            a list)\n |          - `mergeCombiners`, to combine two C's into a single one (e.g., merges\n |            the lists)\n |      \n |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n |      modify and return their first argument instead of creating a new C.\n |      \n |      In addition, users can control the partitioning of the output RDD.\n |      \n |      Notes\n |      -----\n |      V and C can be different -- for example, one might group an RDD of type\n |          (Int, Int) into an RDD of type (Int, List[Int]).\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n |      >>> def to_list(a):\n |      ...     return [a]\n |      ...\n |      >>> def append(a, b):\n |      ...     a.append(b)\n |      ...     return a\n |      ...\n |      >>> def extend(a, b):\n |      ...     a.extend(b)\n |      ...     return a\n |      ...\n |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n |      [('a', [1, 2]), ('b', [1])]\n |  \n |  count(self) -> int\n |      Return the number of elements in this RDD.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4]).count()\n |      3\n |  \n |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n |      Approximate version of count() that returns a potentially incomplete\n |      result within a timeout, even if not all tasks have finished.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(1000), 10)\n |      >>> rdd.countApprox(1000, 1.0)\n |      1000\n |  \n |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n |      Return approximate number of distinct elements in the RDD.\n |      \n |      Parameters\n |      ----------\n |      relativeSD : float, optional\n |          Relative accuracy. Smaller values create\n |          counters that require more space.\n |          It must be greater than 0.000017.\n |      \n |      Notes\n |      -----\n |      The algorithm used is based on streamlib's implementation of\n |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n |      of The Art Cardinality Estimation Algorithm\", available here\n |      <https://doi.org/10.1145/2452376.2452456>`_.\n |      \n |      Examples\n |      --------\n |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n |      >>> 900 < n < 1100\n |      True\n |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n |      >>> 16 < n < 24\n |      True\n |  \n |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n |      Count the number of elements for each key, and return the result to the\n |      master as a dictionary.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n |      >>> sorted(rdd.countByKey().items())\n |      [('a', 2), ('b', 1)]\n |  \n |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n |      Return the count of each unique value in this RDD as a dictionary of\n |      (value, count) pairs.\n |      \n |      Examples\n |      --------\n |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n |      [(1, 2), (2, 3)]\n |  \n |  distinct(self: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Return a new RDD containing the distinct elements in this RDD.\n |      \n |      Examples\n |      --------\n |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n |      [1, 2, 3]\n |  \n |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n |      Return a new RDD containing only the elements that satisfy a predicate.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n |      [2, 4]\n |  \n |  first(self: 'RDD[T]') -> ~T\n |      Return the first element in this RDD.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4]).first()\n |      2\n |      >>> sc.parallelize([]).first()\n |      Traceback (most recent call last):\n |          ...\n |      ValueError: RDD is empty\n |  \n |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n |      Return a new RDD by first applying a function to all elements of this\n |      RDD, and then flattening the results.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([2, 3, 4])\n |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n |      [1, 1, 1, 2, 2, 3]\n |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n |  \n |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n |      Pass each value in the key-value pair RDD through a flatMap function\n |      without changing the keys; this also retains the original RDD's\n |      partitioning.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n |      >>> def f(x): return x\n |      >>> x.flatMapValues(f).collect()\n |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n |  \n |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given associative function and a neutral \"zero value.\"\n |      \n |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      This behaves somewhat differently from fold operations implemented\n |      for non-distributed collections in functional languages like Scala.\n |      This fold operation may be applied to partitions individually, and then\n |      fold those results into the final result, rather than apply the fold\n |      to each element sequentially in some defined ordering. For functions\n |      that are not commutative, the result may differ from that of a fold\n |      applied to a non-distributed collection.\n |      \n |      Examples\n |      --------\n |      >>> from operator import add\n |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n |      15\n |  \n |  foldByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~V, func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, V]]'\n |      Merge the values for each key using an associative function \"func\"\n |      and a neutral \"zeroValue\" which may be added to the result an\n |      arbitrary number of times, and must not change the result\n |      (e.g., 0 for addition, or 1 for multiplication.).\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n |      >>> from operator import add\n |      >>> sorted(rdd.foldByKey(0, add).collect())\n |      [('a', 2), ('b', 1)]\n |  \n |  foreach(self: 'RDD[T]', f: Callable[[~T], NoneType]) -> None\n |      Applies a function to all elements of this RDD.\n |      \n |      Examples\n |      --------\n |      >>> def f(x): print(x)\n |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n |  \n |  foreachPartition(self: 'RDD[T]', f: Callable[[Iterable[~T]], NoneType]) -> None\n |      Applies a function to each partition of this RDD.\n |      \n |      Examples\n |      --------\n |      >>> def f(iterator):\n |      ...     for x in iterator:\n |      ...          print(x)\n |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n |  \n |  fullOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]'\n |      Perform a right outer join of `self` and `other`.\n |      \n |      For each element (k, v) in `self`, the resulting RDD will either\n |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n |      (k, (v, None)) if no elements in `other` have key k.\n |      \n |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n |      (k, (None, w)) if no elements in `self` have key k.\n |      \n |      Hash-partitions the resulting RDD into the given number of partitions.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n |      >>> sorted(x.fullOuterJoin(y).collect())\n |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n |  \n |  getCheckpointFile(self) -> Optional[str]\n |      Gets the name of the file to which this RDD was checkpointed\n |      \n |      Not defined if RDD is checkpointed locally.\n |  \n |  getNumPartitions(self) -> int\n |      Returns the number of partitions in RDD\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n |      >>> rdd.getNumPartitions()\n |      2\n |  \n |  getResourceProfile(self) -> Optional[pyspark.resource.profile.ResourceProfile]\n |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n |      if it wasn't specified.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Returns\n |      -------\n |      :py:class:`pyspark.resource.ResourceProfile`\n |          The user specified profile or None if none were specified\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |  \n |  getStorageLevel(self) -> pyspark.storagelevel.StorageLevel\n |      Get the RDD's current storage level.\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([1,2])\n |      >>> rdd1.getStorageLevel()\n |      StorageLevel(False, False, False, False, 1)\n |      >>> print(rdd1.getStorageLevel())\n |      Serialized 1x Replicated\n |  \n |  glom(self: 'RDD[T]') -> 'RDD[List[T]]'\n |      Return an RDD created by coalescing all elements within each partition\n |      into a list.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n |      >>> sorted(rdd.glom().collect())\n |      [[1, 2], [3, 4]]\n |  \n |  groupBy(self: 'RDD[T]', f: Callable[[~T], ~K], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, Iterable[T]]]'\n |      Return an RDD of grouped items.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n |  \n |  groupByKey(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, Iterable[V]]]'\n |      Group the values for each key in the RDD into a single sequence.\n |      Hash-partitions the resulting RDD with numPartitions partitions.\n |      \n |      Notes\n |      -----\n |      If you are grouping in order to perform an aggregation (such as a\n |      sum or average) over each key, using reduceByKey or aggregateByKey will\n |      provide much better performance.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n |      [('a', 2), ('b', 1)]\n |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n |      [('a', [1, 1]), ('b', [1])]\n |  \n |  groupWith(self: 'RDD[Tuple[Any, Any]]', other: 'RDD[Tuple[Any, Any]]', *others: 'RDD[Tuple[Any, Any]]') -> 'RDD[Tuple[Any, Tuple[ResultIterable[Any], ...]]]'\n |      Alias for cogroup but with support for multiple RDDs.\n |      \n |      Examples\n |      --------\n |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> y = sc.parallelize([(\"a\", 2)])\n |      >>> z = sc.parallelize([(\"b\", 42)])\n |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n |  \n |  histogram(self: 'RDD[S]', buckets: Union[int, List[ForwardRef('S')], Tuple[ForwardRef('S'), ...]]) -> Tuple[Sequence[ForwardRef('S')], List[int]]\n |      Compute a histogram using the provided buckets. The buckets\n |      are all open to the right except for the last which is closed.\n |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n |      and 50 we would have a histogram of 1,0,1.\n |      \n |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n |      this can be switched from an O(log n) insertion to O(1) per\n |      element (where n is the number of buckets).\n |      \n |      Buckets must be sorted, not contain any duplicates, and have\n |      at least two elements.\n |      \n |      If `buckets` is a number, it will generate buckets which are\n |      evenly spaced between the minimum and maximum of the RDD. For\n |      example, if the min value is 0 and the max is 100, given `buckets`\n |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n |      be at least 1. An exception is raised if the RDD contains infinity.\n |      If the elements in the RDD do not vary (max == min), a single bucket\n |      will be used.\n |      \n |      The return value is a tuple of buckets and histogram.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(51))\n |      >>> rdd.histogram(2)\n |      ([0, 25, 50], [25, 26])\n |      >>> rdd.histogram([0, 5, 25, 50])\n |      ([0, 5, 25, 50], [5, 20, 26])\n |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n |      (('a', 'b', 'c'), [2, 2])\n |  \n |  id(self) -> int\n |      A unique ID for this RDD (within its SparkContext).\n |  \n |  intersection(self: 'RDD[T]', other: 'RDD[T]') -> 'RDD[T]'\n |      Return the intersection of this RDD and another one. The output will\n |      not contain any duplicate elements, even if the input RDDs did.\n |      \n |      Notes\n |      -----\n |      This method performs a shuffle internally.\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n |      >>> rdd1.intersection(rdd2).collect()\n |      [1, 2, 3]\n |  \n |  isCheckpointed(self) -> bool\n |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n |  \n |  isEmpty(self) -> bool\n |      Returns true if and only if the RDD contains no elements at all.\n |      \n |      Notes\n |      -----\n |      An RDD may be empty even when it has at least 1 partition.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([]).isEmpty()\n |      True\n |      >>> sc.parallelize([1]).isEmpty()\n |      False\n |  \n |  isLocallyCheckpointed(self) -> bool\n |      Return whether this RDD is marked for local checkpointing.\n |      \n |      Exposed for testing.\n |  \n |  join(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, U]]]'\n |      Return an RDD containing all pairs of elements with matching keys in\n |      `self` and `other`.\n |      \n |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n |      (k, v1) is in `self` and (k, v2) is in `other`.\n |      \n |      Performs a hash join across the cluster.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n |      >>> sorted(x.join(y).collect())\n |      [('a', (1, 2)), ('a', (1, 3))]\n |  \n |  keyBy(self: 'RDD[T]', f: Callable[[~T], ~K]) -> 'RDD[Tuple[K, T]]'\n |      Creates tuples of the elements in this RDD by applying `f`.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n the total\n |      count of the given :class:`DataFrame`.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(100), 4)\n |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n |      True\n |  \n |  sampleByKey(self: 'RDD[Tuple[K, V]]', withReplacement: bool, fractions: Dict[~K, Union[float, int]], seed: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n |      Return a subset of this RDD sampled by key (via stratified sampling).\n |      Create a sample of this RDD using variable sampling rates for\n |      different keys as specified by fractions, a key to sampling rate map.\n |      \n |      Examples\n |      --------\n |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n |      True\n |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n |      True\n |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n |      True\n |  \n |  sampleStdev(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Compute the sample standard deviation of this RDD's elements (which\n |      corrects for bias in estimating the standard deviation by dividing by\n |      N-1 instead of N).\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n |      1.0\n |  \n |  sampleVariance(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Compute the sample variance of this RDD's elements (which corrects\n |      for bias in estimating the variance by dividing by N-1 instead of N).\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n |      1.0\n |  \n |  saveAsHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n |      converted for output using either user specified converters or, by default,\n |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n |      \n |      Parameters\n |      ----------\n |      conf : dict\n |          Hadoop job configuration\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |  \n |  saveAsHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, compressionCodecClass: Optional[str] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n |      will be inferred if not specified. Keys and values are converted for output using either\n |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to Hadoop file\n |      outputFormatClass : str\n |          fully qualified classname of Hadoop OutputFormat\n |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n |      keyClass : str, optional\n |          fully qualified classname of key Writable class\n |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n |      valueClass : str, optional\n |          fully qualified classname of value Writable class\n |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |      conf : dict, optional\n |          (None by default)\n |      compressionCodecClass : str\n |          fully qualified classname of the compression codec class\n |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n |  \n |  saveAsNewAPIHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n |      converted for output using either user specified converters or, by default,\n |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n |      \n |      Parameters\n |      ----------\n |      conf : dict\n |          Hadoop job configuration\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |  \n |  saveAsNewAPIHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n |      will be inferred if not specified. Keys and values are converted for output using either\n |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n |      \n |      path : str\n |          path to Hadoop file\n |      outputFormatClass : str\n |          fully qualified classname of Hadoop OutputFormat\n |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n |      keyClass : str, optional\n |          fully qualified classname of key Writable class\n |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n |      valueClass : str, optional\n |          fully qualified classname of value Writable class\n |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |      conf : dict, optional\n |          Hadoop job configuration (None by default)\n |  \n |  saveAsPickleFile(self, path: str, batchSize: int = 10) -> None\n |      Save this RDD as a SequenceFile of serialized objects. The serializer\n |      used is :class:`pyspark.serializers.CPickleSerializer`, default batch size\n |      is 10.\n |      \n |      Examples\n |      --------\n |      >>> from tempfile import NamedTemporaryFile\n |      >>> tmpFile = NamedTemporaryFile(delete=True)\n |      >>> tmpFile.close()\n |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n |      ['1', '2', 'rdd', 'spark']\n |  \n |  saveAsSequenceFile(self: 'RDD[Tuple[K, V]]', path: str, compressionCodecClass: Optional[str] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n |      RDD's key and value types. The mechanism is as follows:\n |      \n |          1. Pickle is used to convert pickled Python RDD into RDD of Java objects.\n |          2. Keys and values of this Java RDD are converted to Writables and written out.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to sequence file\n |      compressionCodecClass : str, optional\n |          fully qualified classname of the compression codec class\n |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n |  \n |  saveAsTextFile(self, path: str, compressionCodecClass: Optional[str] = None) -> None\n |      Save this RDD as a text file, using string representations of elements.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to text file\n |      compressionCodecClass : str, optional\n |          fully qualified classname of the compression codec class\n |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n |      \n |      Examples\n |      --------\n |      >>> from tempfile import NamedTemporaryFile\n |      >>> tempFile = NamedTemporaryFile(delete=True)\n |      >>> tempFile.close()\n |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n |      >>> from fileinput import input\n |      >>> from glob import glob\n |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n |      \n |      Empty lines are tolerated when saving to text files.\n |      \n |      >>> from tempfile import NamedTemporaryFile\n |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n |      >>> tempFile2.close()\n |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n |      '\\n\\n\\nbar\\nfoo\\n'\n |      \n |      Using compressionCodecClass\n |      \n |      >>> from tempfile import NamedTemporaryFile\n |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n |      >>> tempFile3.close()\n |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n |      >>> from fileinput import input, hook_compressed\n |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n |      >>> ''.join([r.decode('utf-8') if isinstance(r, bytes) else r for r in result])\n |      'bar\\nfoo\\n'\n |  \n |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n |      Assign a name to this RDD.\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([1, 2])\n |      >>> rdd1.setName('RDD1').name()\n |      'RDD1'\n |  \n |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Sorts this RDD by the given keyfunc\n |      \n |      Examples\n |      --------\n |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |  \n |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f9dfd275f70>) -> 'RDD[Tuple[K, V]]'\n |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n |      \n |      Examples\n |      --------\n |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |      >>> sc.parallelize(tmp).sortByKey().first()\n |      ('1', 3)\n |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n |  \n |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n |      Return a :class:`StatCounter` object that captures the mean, variance\n |      and count of the RDD's elements in one operation.\n |  \n |  stdev(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Compute the standard deviation of this RDD's elements.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).stdev()\n |      0.816...\n |  \n |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Return each value in `self` that is not contained in `other`.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n |      >>> sorted(x.subtract(y).collect())\n |      [('a', 1), ('b', 4), ('b', 5)]\n |  \n |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n |      Return each (key, value) pair in `self` that has no pair with matching\n |      key in `other`.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n |      >>> sorted(x.subtractByKey(y).collect())\n |      [('b', 4), ('b', 5)]\n |  \n |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Add up the elements in this RDD.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n |      6.0\n |  \n |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n |      Approximate operation to return the sum within a timeout\n |      or meet the confidence.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(1000), 10)\n |      >>> r = sum(range(1000))\n |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n |      True\n |  \n |  take(self: 'RDD[T]', num: int) -> List[~T]\n |      Take the first num elements of the RDD.\n |      \n |      It works by first scanning one partition, and use the results from\n |      that partition to estimate the number of additional partitions needed\n |      to satisfy the limit.\n |      \n |      Translated from the Scala implementation in RDD#take().\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n |      [2, 3]\n |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n |      [2, 3, 4, 5, 6]\n |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n |      [91, 92, 93]\n |  \n |  takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n |      Get the N elements from an RDD ordered in ascending order or as\n |      specified by the optional key function.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n |      [1, 2, 3, 4, 5, 6]\n |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n |      [10, 9, 7, 6, 5, 4]\n |  \n |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int] = None) -> List[~T]\n |      Return a fixed-size sampled subset of this RDD.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(0, 10))\n |      >>> len(rdd.takeSample(True, 20, 1))\n |      20\n |      >>> len(rdd.takeSample(False, 5, 2))\n |      5\n |      >>> len(rdd.takeSample(False, 15, 3))\n |      10\n |  \n |  toDF(self, schema=None, sampleRatio=None)\n |      Converts current :class:`RDD` into a :class:`DataFrame`\n |      \n |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n |      \n |      Parameters\n |      ----------\n |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n |          column names, default is None.  The data type string format equals to\n |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n |      sampleRatio : float, optional\n |          the sample ratio of rows used for inferring\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      >>> rdd.toDF().collect()\n |      [Row(name='Alice', age=1)]\n |  \n |  toDebugString(self) -> Optional[bytes]\n |      A description of this RDD and its recursive dependencies for debugging.\n |  \n |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n |      Return an iterator that contains all of the elements in this RDD.\n |      The iterator will consume as much memory as the largest partition in this RDD.\n |      With prefetch it may consume up to the memory of the 2 largest partitions.\n |      \n |      Parameters\n |      ----------\n |      prefetchPartitions : bool, optional\n |          If Spark should pre-fetch the next partition\n |          before it is needed.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(10))\n |      >>> [x for x in rdd.toLocalIterator()]\n |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n |  \n |  top(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n |      Get the top N elements from an RDD.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      It returns the list sorted in descending order.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n |      [12]\n |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n |      [6, 5]\n |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n |      [4, 3, 2]\n |  \n |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n |      Aggregates the elements of this RDD in a multi-level tree\n |      pattern.\n |      \n |      depth : int, optional\n |          suggested depth of the tree (default: 2)\n |      \n |      Examples\n |      --------\n |      >>> add = lambda x, y: x + y\n |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      >>> rdd.treeAggregate(0, add, add)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 1)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 2)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 5)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 10)\n |      -5\n |  \n |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n |      Reduces the elements of this RDD in a multi-level tree pattern.\n |      \n |      Parameters\n |      ----------\n |      f : function\n |      depth : int, optional\n |          suggested depth of the tree (default: 2)\n |      \n |      Examples\n |      --------\n |      >>> add = lambda x, y: x + y\n |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      >>> rdd.treeReduce(add)\n |      -5\n |      >>> rdd.treeReduce(add, 1)\n |      -5\n |      >>> rdd.treeReduce(add, 2)\n |      -5\n |      >>> rdd.treeReduce(add, 5)\n |      -5\n |      >>> rdd.treeReduce(add, 10)\n |      -5\n |  \n |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n |      Return the union of this RDD and another one.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n |      >>> rdd.union(rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n |      Mark the RDD as non-persistent, and remove all blocks for it from\n |      memory and disk.\n |      \n |      .. versionchanged:: 3.0.0\n |         Added optional argument `blocking` to specify whether to block until all\n |         blocks are deleted.\n |  \n |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n |      Return an RDD with the values of each tuple.\n |      \n |      Examples\n |      --------\n |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n |      >>> m.collect()\n |      [2, 4]\n |  \n |  variance(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Compute the variance of this RDD's elements.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).variance()\n |      0.666...\n |  \n |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n |      This is only supported on certain cluster managers and currently requires dynamic\n |      allocation to be enabled. It will result in new executors with the resources specified\n |      being acquired to calculate the RDD.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |  \n |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n |      Zips this RDD with another one, returning key-value pairs with the\n |      first element in each RDD second element in each RDD, etc. Assumes\n |      that the two RDDs have the same number of partitions and the same\n |      number of elements in each partition (e.g. one was made through\n |      a map on the other).\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize(range(0,5))\n |      >>> y = sc.parallelize(range(1000, 1005))\n |      >>> x.zip(y).collect()\n |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n |  \n |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n |      Zips this RDD with its element indices.\n |      \n |      The ordering is first based on the partition index and then the\n |      ordering of items within each partition. So the first item in\n |      the first partition gets index 0, and the last item in the last\n |      partition receives the largest index.\n |      \n |      This method needs to trigger a spark job when this RDD contains\n |      more than one partitions.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n |  \n |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n |      Zips this RDD with generated unique Long ids.\n |      \n |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n |      n is the number of partitions. So there may exist gaps, but this\n |      method won't trigger a spark job, which is different from\n |      :meth:`zipWithIndex`.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  context\n |      The :class:`SparkContext` that this RDD was created on.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __orig_bases__ = (typing.Generic[+T_co],)\n |  \n |  __parameters__ = (+T_co,)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from typing.Generic:\n |  \n |  __class_getitem__(params) from builtins.type\n |  \n |  __init_subclass__(*args, **kwargs) from builtins.type\n |      This method is called when a class is subclassed.\n |      \n |      The default implementation does nothing. It may be\n |      overridden to extend subclasses.\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Help on RDD in module pyspark.rdd object:\n\nclass RDD(typing.Generic)\n |  RDD(jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n |  \n |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n |  Represents an immutable, partitioned collection of elements that can be\n |  operated on in parallel.\n |  \n |  Method resolution order:\n |      RDD\n |      typing.Generic\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n |      Return the union of this RDD and another one.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n |      >>> (rdd + rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  __getnewargs__(self) -> NoReturn\n |  \n |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given combine functions and a neutral \"zero\n |      value.\"\n |      \n |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      The first function (seqOp) can return a different result type, U, than\n |      the type of this RDD. Thus, we need one operation for merging a T into\n |      an U and one operation for merging two U\n |      \n |      Examples\n |      --------\n |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n |      (10, 4)\n |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n |      (0, 0)\n |  \n |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, U]]'\n |      Aggregate the values of each key, using given combine functions and a neutral\n |      \"zero value\". This function can return a different result type, U, than the type\n |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n |      a U and one operation for merging two U's, The former operation is used for merging\n |      values within a partition, and the latter is used for merging values between\n |      partitions. To avoid memory allocation, both of these functions are\n |      allowed to modify and return their first argument instead of creating a new U.\n |  \n |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n |      entire stage and relaunch all tasks for this stage.\n |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Returns\n |      -------\n |      :class:`RDDBarrier`\n |          instance that provides actions within a barrier stage.\n |      \n |      See Also\n |      --------\n |      pyspark.BarrierTaskContext\n |      \n |      Notes\n |      -----\n |      For additional information see\n |      \n |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n |      \n |      This API is experimental\n |  \n |  cache(self: 'RDD[T]') -> 'RDD[T]'\n |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n |  \n |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n |      Return the Cartesian product of this RDD and another one, that is, the\n |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n |      ``b`` is in `other`.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2])\n |      >>> sorted(rdd.cartesian(rdd).collect())\n |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n |  \n |  checkpoint(self) -> None\n |      Mark this RDD for checkpointing. It will be saved to a file inside the\n |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n |      all references to its parent RDDs will be removed. This function must\n |      be called before any job has been executed on this RDD. It is strongly\n |      recommended that this RDD is persisted in memory, otherwise saving it\n |      on a file will require recomputation.\n |  \n |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n |      Removes an RDD's shuffles and it's non-persisted ancestors.\n |      \n |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n |      If you use the RDD after this call, you should checkpoint and materialize it first.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      Parameters\n |      ----------\n |      blocking : bool, optional\n |         block on shuffle cleanup tasks. Disabled by default.\n |      \n |      Notes\n |      -----\n |      This API is a developer API.\n |  \n |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n |      Return a new RDD that is reduced into `numPartitions` partitions.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n |      [[1], [2, 3], [4, 5]]\n |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n |      [[1, 2, 3, 4, 5]]\n |  \n |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n |      For each key k in `self` or `other`, return a resulting RDD that\n |      contains a tuple with the list of values for that key in `self` as\n |      well as `other`.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> y = sc.parallelize([(\"a\", 2)])\n |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n |      [('a', ([1], [2])), ('b', ([4], []))]\n |  \n |  collect(self: 'RDD[T]') -> List[~T]\n |      Return a list that contains all of the elements in this RDD.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |  \n |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n |      Return the key-value pairs in this RDD to the master as a dictionary.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting data is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n |      >>> m[1]\n |      2\n |      >>> m[3]\n |      4\n |  \n |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n |      When collect rdd, use this method to specify job group.\n |      \n |      .. versionadded:: 3.0.0\n |      .. deprecated:: 3.1.0\n |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n |  \n |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, U]]'\n |      Generic function to combine the elements for each key using a custom\n |      set of aggregation functions.\n |      \n |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n |      type\" C.\n |      \n |      Users provide three functions:\n |      \n |          - `createCombiner`, which turns a V into a C (e.g., creates\n |            a one-element list)\n |          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n |            a list)\n |          - `mergeCombiners`, to combine two C's into a single one (e.g., merges\n |            the lists)\n |      \n |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n |      modify and return their first argument instead of creating a new C.\n |      \n |      In addition, users can control the partitioning of the output RDD.\n |      \n |      Notes\n |      -----\n |      V and C can be different -- for example, one might group an RDD of type\n |          (Int, Int) into an RDD of type (Int, List[Int]).\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n |      >>> def to_list(a):\n |      ...     return [a]\n |      ...\n |      >>> def append(a, b):\n |      ...     a.append(b)\n |      ...     return a\n |      ...\n |      >>> def extend(a, b):\n |      ...     a.extend(b)\n |      ...     return a\n |      ...\n |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n |      [('a', [1, 2]), ('b', [1])]\n |  \n |  count(self) -> int\n |      Return the number of elements in this RDD.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4]).count()\n |      3\n |  \n |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n |      Approximate version of count() that returns a potentially incomplete\n |      result within a timeout, even if not all tasks have finished.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(1000), 10)\n |      >>> rdd.countApprox(1000, 1.0)\n |      1000\n |  \n |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n |      Return approximate number of distinct elements in the RDD.\n |      \n |      Parameters\n |      ----------\n |      relativeSD : float, optional\n |          Relative accuracy. Smaller values create\n |          counters that require more space.\n |          It must be greater than 0.000017.\n |      \n |      Notes\n |      -----\n |      The algorithm used is based on streamlib's implementation of\n |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n |      of The Art Cardinality Estimation Algorithm\", available here\n |      <https://doi.org/10.1145/2452376.2452456>`_.\n |      \n |      Examples\n |      --------\n |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n |      >>> 900 < n < 1100\n |      True\n |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n |      >>> 16 < n < 24\n |      True\n |  \n |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n |      Count the number of elements for each key, and return the result to the\n |      master as a dictionary.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n |      >>> sorted(rdd.countByKey().items())\n |      [('a', 2), ('b', 1)]\n |  \n |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n |      Return the count of each unique value in this RDD as a dictionary of\n |      (value, count) pairs.\n |      \n |      Examples\n |      --------\n |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n |      [(1, 2), (2, 3)]\n |  \n |  distinct(self: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Return a new RDD containing the distinct elements in this RDD.\n |      \n |      Examples\n |      --------\n |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n |      [1, 2, 3]\n |  \n |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n |      Return a new RDD containing only the elements that satisfy a predicate.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n |      [2, 4]\n |  \n |  first(self: 'RDD[T]') -> ~T\n |      Return the first element in this RDD.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4]).first()\n |      2\n |      >>> sc.parallelize([]).first()\n |      Traceback (most recent call last):\n |          ...\n |      ValueError: RDD is empty\n |  \n |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n |      Return a new RDD by first applying a function to all elements of this\n |      RDD, and then flattening the results.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([2, 3, 4])\n |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n |      [1, 1, 1, 2, 2, 3]\n |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n |  \n |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n |      Pass each value in the key-value pair RDD through a flatMap function\n |      without changing the keys; this also retains the original RDD's\n |      partitioning.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n |      >>> def f(x): return x\n |      >>> x.flatMapValues(f).collect()\n |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n |  \n |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given associative function and a neutral \"zero value.\"\n |      \n |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      This behaves somewhat differently from fold operations implemented\n |      for non-distributed collections in functional languages like Scala.\n |      This fold operation may be applied to partitions individually, and then\n |      fold those results into the final result, rather than apply the fold\n |      to each element sequentially in some defined ordering. For functions\n |      that are not commutative, the result may differ from that of a fold\n |      applied to a non-distributed collection.\n |      \n |      Examples\n |      --------\n |      >>> from operator import add\n |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n |      15\n |  \n |  foldByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~V, func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, V]]'\n |      Merge the values for each key using an associative function \"func\"\n |      and a neutral \"zeroValue\" which may be added to the result an\n |      arbitrary number of times, and must not change the result\n |      (e.g., 0 for addition, or 1 for multiplication.).\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n |      >>> from operator import add\n |      >>> sorted(rdd.foldByKey(0, add).collect())\n |      [('a', 2), ('b', 1)]\n |  \n |  foreach(self: 'RDD[T]', f: Callable[[~T], NoneType]) -> None\n |      Applies a function to all elements of this RDD.\n |      \n |      Examples\n |      --------\n |      >>> def f(x): print(x)\n |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n |  \n |  foreachPartition(self: 'RDD[T]', f: Callable[[Iterable[~T]], NoneType]) -> None\n |      Applies a function to each partition of this RDD.\n |      \n |      Examples\n |      --------\n |      >>> def f(iterator):\n |      ...     for x in iterator:\n |      ...          print(x)\n |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n |  \n |  fullOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]'\n |      Perform a right outer join of `self` and `other`.\n |      \n |      For each element (k, v) in `self`, the resulting RDD will either\n |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n |      (k, (v, None)) if no elements in `other` have key k.\n |      \n |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n |      (k, (None, w)) if no elements in `self` have key k.\n |      \n |      Hash-partitions the resulting RDD into the given number of partitions.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n |      >>> sorted(x.fullOuterJoin(y).collect())\n |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n |  \n |  getCheckpointFile(self) -> Optional[str]\n |      Gets the name of the file to which this RDD was checkpointed\n |      \n |      Not defined if RDD is checkpointed locally.\n |  \n |  getNumPartitions(self) -> int\n |      Returns the number of partitions in RDD\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n |      >>> rdd.getNumPartitions()\n |      2\n |  \n |  getResourceProfile(self) -> Optional[pyspark.resource.profile.ResourceProfile]\n |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n |      if it wasn't specified.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Returns\n |      -------\n |      :py:class:`pyspark.resource.ResourceProfile`\n |          The user specified profile or None if none were specified\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |  \n |  getStorageLevel(self) -> pyspark.storagelevel.StorageLevel\n |      Get the RDD's current storage level.\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([1,2])\n |      >>> rdd1.getStorageLevel()\n |      StorageLevel(False, False, False, False, 1)\n |      >>> print(rdd1.getStorageLevel())\n |      Serialized 1x Replicated\n |  \n |  glom(self: 'RDD[T]') -> 'RDD[List[T]]'\n |      Return an RDD created by coalescing all elements within each partition\n |      into a list.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n |      >>> sorted(rdd.glom().collect())\n |      [[1, 2], [3, 4]]\n |  \n |  groupBy(self: 'RDD[T]', f: Callable[[~T], ~K], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, Iterable[T]]]'\n |      Return an RDD of grouped items.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n |  \n |  groupByKey(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f9e0569e5e0>) -> 'RDD[Tuple[K, Iterable[V]]]'\n |      Group the values for each key in the RDD into a single sequence.\n |      Hash-partitions the resulting RDD with numPartitions partitions.\n |      \n |      Notes\n |      -----\n |      If you are grouping in order to perform an aggregation (such as a\n |      sum or average) over each key, using reduceByKey or aggregateByKey will\n |      provide much better performance.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n |      [('a', 2), ('b', 1)]\n |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n |      [('a', [1, 1]), ('b', [1])]\n |  \n |  groupWith(self: 'RDD[Tuple[Any, Any]]', other: 'RDD[Tuple[Any, Any]]', *others: 'RDD[Tuple[Any, Any]]') -> 'RDD[Tuple[Any, Tuple[ResultIterable[Any], ...]]]'\n |      Alias for cogroup but with support for multiple RDDs.\n |      \n |      Examples\n |      --------\n |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> y = sc.parallelize([(\"a\", 2)])\n |      >>> z = sc.parallelize([(\"b\", 42)])\n |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n |  \n |  histogram(self: 'RDD[S]', buckets: Union[int, List[ForwardRef('S')], Tuple[ForwardRef('S'), ...]]) -> Tuple[Sequence[ForwardRef('S')], List[int]]\n |      Compute a histogram using the provided buckets. The buckets\n |      are all open to the right except for the last which is closed.\n |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n |      and 50 we would have a histogram of 1,0,1.\n |      \n |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n |      this can be switched from an O(log n) insertion to O(1) per\n |      element (where n is the number of buckets).\n |      \n |      Buckets must be sorted, not contain any duplicates, and have\n |      at least two elements.\n |      \n |      If `buckets` is a number, it will generate buckets which are\n |      evenly spaced between the minimum and maximum of the RDD. For\n |      example, if the min value is 0 and the max is 100, given `buckets`\n |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n |      be at least 1. An exception is raised if the RDD contains infinity.\n |      If the elements in the RDD do not vary (max == min), a single bucket\n |      will be used.\n |      \n |      The return value is a tuple of buckets and histogram.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(51))\n |      >>> rdd.histogram(2)\n |      ([0, 25, 50], [25, 26])\n |      >>> rdd.histogram([0, 5, 25, 50])\n |      ([0, 5, 25, 50], [5, 20, 26])\n |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n |      (('a', 'b', 'c'), [2, 2])\n |  \n |  id(self) -> int\n |      A unique ID for this RDD (within its SparkContext).\n |  \n |  intersection(self: 'RDD[T]', other: 'RDD[T]') -> 'RDD[T]'\n |      Return the intersection of this RDD and another one. The output will\n |      not contain any duplicate elements, even if the input RDDs did.\n |      \n |      Notes\n |      -----\n |      This method performs a shuffle internally.\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n |      >>> rdd1.intersection(rdd2).collect()\n |      [1, 2, 3]\n |  \n |  isCheckpointed(self) -> bool\n |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n |  \n |  isEmpty(self) -> bool\n |      Returns true if and only if the RDD contains no elements at all.\n |      \n |      Notes\n |      -----\n |      An RDD may be empty even when it has at least 1 partition.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([]).isEmpty()\n |      True\n |      >>> sc.parallelize([1]).isEmpty()\n |      False\n |  \n |  isLocallyCheckpointed(self) -> bool\n |      Return whether this RDD is marked for local checkpointing.\n |      \n |      Exposed for testing.\n |  \n |  join(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, U]]]'\n |      Return an RDD containing all pairs of elements with matching keys in\n |      `self` and `other`.\n |      \n |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n |      (k, v1) is in `self` and (k, v2) is in `other`.\n |      \n |      Performs a hash join across the cluster.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n |      >>> sorted(x.join(y).collect())\n |      [('a', (1, 2)), ('a', (1, 3))]\n |  \n |  keyBy(self: 'RDD[T]', f: Callable[[~T], ~K]) -> 'RDD[Tuple[K, T]]'\n |      Creates tuples of the elements in this RDD by applying `f`.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n the total\n |      count of the given :class:`DataFrame`.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(100), 4)\n |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n |      True\n |  \n |  sampleByKey(self: 'RDD[Tuple[K, V]]', withReplacement: bool, fractions: Dict[~K, Union[float, int]], seed: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n |      Return a subset of this RDD sampled by key (via stratified sampling).\n |      Create a sample of this RDD using variable sampling rates for\n |      different keys as specified by fractions, a key to sampling rate map.\n |      \n |      Examples\n |      --------\n |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n |      True\n |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n |      True\n |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n |      True\n |  \n |  sampleStdev(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Compute the sample standard deviation of this RDD's elements (which\n |      corrects for bias in estimating the standard deviation by dividing by\n |      N-1 instead of N).\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n |      1.0\n |  \n |  sampleVariance(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Compute the sample variance of this RDD's elements (which corrects\n |      for bias in estimating the variance by dividing by N-1 instead of N).\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n |      1.0\n |  \n |  saveAsHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n |      converted for output using either user specified converters or, by default,\n |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n |      \n |      Parameters\n |      ----------\n |      conf : dict\n |          Hadoop job configuration\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |  \n |  saveAsHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, compressionCodecClass: Optional[str] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n |      will be inferred if not specified. Keys and values are converted for output using either\n |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to Hadoop file\n |      outputFormatClass : str\n |          fully qualified classname of Hadoop OutputFormat\n |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n |      keyClass : str, optional\n |          fully qualified classname of key Writable class\n |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n |      valueClass : str, optional\n |          fully qualified classname of value Writable class\n |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |      conf : dict, optional\n |          (None by default)\n |      compressionCodecClass : str\n |          fully qualified classname of the compression codec class\n |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n |  \n |  saveAsNewAPIHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n |      converted for output using either user specified converters or, by default,\n |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n |      \n |      Parameters\n |      ----------\n |      conf : dict\n |          Hadoop job configuration\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |  \n |  saveAsNewAPIHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n |      will be inferred if not specified. Keys and values are converted for output using either\n |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n |      \n |      path : str\n |          path to Hadoop file\n |      outputFormatClass : str\n |          fully qualified classname of Hadoop OutputFormat\n |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n |      keyClass : str, optional\n |          fully qualified classname of key Writable class\n |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n |      valueClass : str, optional\n |          fully qualified classname of value Writable class\n |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n |      keyConverter : str, optional\n |          fully qualified classname of key converter (None by default)\n |      valueConverter : str, optional\n |          fully qualified classname of value converter (None by default)\n |      conf : dict, optional\n |          Hadoop job configuration (None by default)\n |  \n |  saveAsPickleFile(self, path: str, batchSize: int = 10) -> None\n |      Save this RDD as a SequenceFile of serialized objects. The serializer\n |      used is :class:`pyspark.serializers.CPickleSerializer`, default batch size\n |      is 10.\n |      \n |      Examples\n |      --------\n |      >>> from tempfile import NamedTemporaryFile\n |      >>> tmpFile = NamedTemporaryFile(delete=True)\n |      >>> tmpFile.close()\n |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n |      ['1', '2', 'rdd', 'spark']\n |  \n |  saveAsSequenceFile(self: 'RDD[Tuple[K, V]]', path: str, compressionCodecClass: Optional[str] = None) -> None\n |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n |      RDD's key and value types. The mechanism is as follows:\n |      \n |          1. Pickle is used to convert pickled Python RDD into RDD of Java objects.\n |          2. Keys and values of this Java RDD are converted to Writables and written out.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to sequence file\n |      compressionCodecClass : str, optional\n |          fully qualified classname of the compression codec class\n |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n |  \n |  saveAsTextFile(self, path: str, compressionCodecClass: Optional[str] = None) -> None\n |      Save this RDD as a text file, using string representations of elements.\n |      \n |      Parameters\n |      ----------\n |      path : str\n |          path to text file\n |      compressionCodecClass : str, optional\n |          fully qualified classname of the compression codec class\n |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n |      \n |      Examples\n |      --------\n |      >>> from tempfile import NamedTemporaryFile\n |      >>> tempFile = NamedTemporaryFile(delete=True)\n |      >>> tempFile.close()\n |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n |      >>> from fileinput import input\n |      >>> from glob import glob\n |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n |      \n |      Empty lines are tolerated when saving to text files.\n |      \n |      >>> from tempfile import NamedTemporaryFile\n |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n |      >>> tempFile2.close()\n |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n |      '\\n\\n\\nbar\\nfoo\\n'\n |      \n |      Using compressionCodecClass\n |      \n |      >>> from tempfile import NamedTemporaryFile\n |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n |      >>> tempFile3.close()\n |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n |      >>> from fileinput import input, hook_compressed\n |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n |      >>> ''.join([r.decode('utf-8') if isinstance(r, bytes) else r for r in result])\n |      'bar\\nfoo\\n'\n |  \n |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n |      Assign a name to this RDD.\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([1, 2])\n |      >>> rdd1.setName('RDD1').name()\n |      'RDD1'\n |  \n |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Sorts this RDD by the given keyfunc\n |      \n |      Examples\n |      --------\n |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |  \n |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f9dfd275f70>) -> 'RDD[Tuple[K, V]]'\n |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n |      \n |      Examples\n |      --------\n |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |      >>> sc.parallelize(tmp).sortByKey().first()\n |      ('1', 3)\n |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n |  \n |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n |      Return a :class:`StatCounter` object that captures the mean, variance\n |      and count of the RDD's elements in one operation.\n |  \n |  stdev(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Compute the standard deviation of this RDD's elements.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).stdev()\n |      0.816...\n |  \n |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Return each value in `self` that is not contained in `other`.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n |      >>> sorted(x.subtract(y).collect())\n |      [('a', 1), ('b', 4), ('b', 5)]\n |  \n |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n |      Return each (key, value) pair in `self` that has no pair with matching\n |      key in `other`.\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n |      >>> sorted(x.subtractByKey(y).collect())\n |      [('b', 4), ('b', 5)]\n |  \n |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Add up the elements in this RDD.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n |      6.0\n |  \n |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n |      Approximate operation to return the sum within a timeout\n |      or meet the confidence.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(1000), 10)\n |      >>> r = sum(range(1000))\n |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n |      True\n |  \n |  take(self: 'RDD[T]', num: int) -> List[~T]\n |      Take the first num elements of the RDD.\n |      \n |      It works by first scanning one partition, and use the results from\n |      that partition to estimate the number of additional partitions needed\n |      to satisfy the limit.\n |      \n |      Translated from the Scala implementation in RDD#take().\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n |      [2, 3]\n |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n |      [2, 3, 4, 5, 6]\n |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n |      [91, 92, 93]\n |  \n |  takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n |      Get the N elements from an RDD ordered in ascending order or as\n |      specified by the optional key function.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n |      [1, 2, 3, 4, 5, 6]\n |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n |      [10, 9, 7, 6, 5, 4]\n |  \n |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int] = None) -> List[~T]\n |      Return a fixed-size sampled subset of this RDD.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(0, 10))\n |      >>> len(rdd.takeSample(True, 20, 1))\n |      20\n |      >>> len(rdd.takeSample(False, 5, 2))\n |      5\n |      >>> len(rdd.takeSample(False, 15, 3))\n |      10\n |  \n |  toDF(self, schema=None, sampleRatio=None)\n |      Converts current :class:`RDD` into a :class:`DataFrame`\n |      \n |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n |      \n |      Parameters\n |      ----------\n |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n |          column names, default is None.  The data type string format equals to\n |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n |      sampleRatio : float, optional\n |          the sample ratio of rows used for inferring\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      >>> rdd.toDF().collect()\n |      [Row(name='Alice', age=1)]\n |  \n |  toDebugString(self) -> Optional[bytes]\n |      A description of this RDD and its recursive dependencies for debugging.\n |  \n |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n |      Return an iterator that contains all of the elements in this RDD.\n |      The iterator will consume as much memory as the largest partition in this RDD.\n |      With prefetch it may consume up to the memory of the 2 largest partitions.\n |      \n |      Parameters\n |      ----------\n |      prefetchPartitions : bool, optional\n |          If Spark should pre-fetch the next partition\n |          before it is needed.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(10))\n |      >>> [x for x in rdd.toLocalIterator()]\n |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n |  \n |  top(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n |      Get the top N elements from an RDD.\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      It returns the list sorted in descending order.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n |      [12]\n |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n |      [6, 5]\n |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n |      [4, 3, 2]\n |  \n |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n |      Aggregates the elements of this RDD in a multi-level tree\n |      pattern.\n |      \n |      depth : int, optional\n |          suggested depth of the tree (default: 2)\n |      \n |      Examples\n |      --------\n |      >>> add = lambda x, y: x + y\n |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      >>> rdd.treeAggregate(0, add, add)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 1)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 2)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 5)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 10)\n |      -5\n |  \n |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n |      Reduces the elements of this RDD in a multi-level tree pattern.\n |      \n |      Parameters\n |      ----------\n |      f : function\n |      depth : int, optional\n |          suggested depth of the tree (default: 2)\n |      \n |      Examples\n |      --------\n |      >>> add = lambda x, y: x + y\n |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      >>> rdd.treeReduce(add)\n |      -5\n |      >>> rdd.treeReduce(add, 1)\n |      -5\n |      >>> rdd.treeReduce(add, 2)\n |      -5\n |      >>> rdd.treeReduce(add, 5)\n |      -5\n |      >>> rdd.treeReduce(add, 10)\n |      -5\n |  \n |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n |      Return the union of this RDD and another one.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n |      >>> rdd.union(rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n |      Mark the RDD as non-persistent, and remove all blocks for it from\n |      memory and disk.\n |      \n |      .. versionchanged:: 3.0.0\n |         Added optional argument `blocking` to specify whether to block until all\n |         blocks are deleted.\n |  \n |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n |      Return an RDD with the values of each tuple.\n |      \n |      Examples\n |      --------\n |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n |      >>> m.collect()\n |      [2, 4]\n |  \n |  variance(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Compute the variance of this RDD's elements.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).variance()\n |      0.666...\n |  \n |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n |      This is only supported on certain cluster managers and currently requires dynamic\n |      allocation to be enabled. It will result in new executors with the resources specified\n |      being acquired to calculate the RDD.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |  \n |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n |      Zips this RDD with another one, returning key-value pairs with the\n |      first element in each RDD second element in each RDD, etc. Assumes\n |      that the two RDDs have the same number of partitions and the same\n |      number of elements in each partition (e.g. one was made through\n |      a map on the other).\n |      \n |      Examples\n |      --------\n |      >>> x = sc.parallelize(range(0,5))\n |      >>> y = sc.parallelize(range(1000, 1005))\n |      >>> x.zip(y).collect()\n |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n |  \n |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n |      Zips this RDD with its element indices.\n |      \n |      The ordering is first based on the partition index and then the\n |      ordering of items within each partition. So the first item in\n |      the first partition gets index 0, and the last item in the last\n |      partition receives the largest index.\n |      \n |      This method needs to trigger a spark job when this RDD contains\n |      more than one partitions.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n |  \n |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n |      Zips this RDD with generated unique Long ids.\n |      \n |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n |      n is the number of partitions. So there may exist gaps, but this\n |      method won't trigger a spark job, which is different from\n |      :meth:`zipWithIndex`.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  context\n |      The :class:`SparkContext` that this RDD was created on.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __orig_bases__ = (typing.Generic[+T_co],)\n |  \n |  __parameters__ = (+T_co,)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from typing.Generic:\n |  \n |  __class_getitem__(params) from builtins.type\n |  \n |  __init_subclass__(*args, **kwargs) from builtins.type\n |      This method is called when a class is subclassed.\n |      \n |      The default implementation does nothing. It may be\n |      overridden to extend subclasses.\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["sc._conf.getAll()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c0ef98a-6a7b-4a46-9dd9-33d8c226882d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[29]: [('spark.databricks.preemption.enabled', 'true'),\n ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'),\n ('spark.driver.tempDirectory', '/local_disk0/tmp'),\n ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'),\n ('spark.databricks.managedCatalog.clientClassName',\n  'com.databricks.managedcatalog.ManagedCatalogClientImpl'),\n ('spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider'),\n ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.retry.limit', '20'),\n ('spark.sql.streaming.checkpointFileManagerClass',\n  'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'),\n ('spark.databricks.service.dbutils.repl.backend',\n  'com.databricks.dbconnect.ReplDBUtils'),\n ('spark.databricks.clusterUsageTags.driverPublicDns',\n  'ec2-54-214-224-53.us-west-2.compute.amazonaws.com'),\n ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'),\n ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'),\n ('spark.databricks.clusterSource', 'UI'),\n ('spark.hadoop.hive.server2.transport.mode', 'http'),\n ('spark.executor.memory', '8278m'),\n ('spark.databricks.clusterUsageTags.driverInstanceId', 'i-0207c768ae41e4ff3'),\n ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.hailEnabled', 'false'),\n ('spark.hadoop.databricks.s3.amazonS3Client.cache.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'),\n ('spark.databricks.clusterUsageTags.containerType', 'LXC'),\n ('spark.eventLog.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'false'),\n ('spark.hadoop.fs.stage.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.hmshandler.retry.interval', '2000'),\n ('spark.executor.tempDirectory', '/local_disk0/tmp'),\n ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'),\n ('spark.hadoop.fs.fcfs-abfss.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.mapred.output.committer.class',\n  'com.databricks.backend.daemon.data.client.DirectOutputCommitter'),\n ('spark.hadoop.hive.server2.thrift.http.port', '10000'),\n ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n ('spark.sql.allowMultipleContexts', 'false'),\n ('spark.databricks.eventLog.enabled', 'true'),\n ('spark.home', '/databricks/spark'),\n ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '0'),\n ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'),\n ('spark.task.reaper.enabled', 'true'),\n ('spark.storage.memoryFraction', '0.5'),\n ('eventLog.rolloverIntervalSeconds', '900'),\n ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '0'),\n ('spark.databricks.sql.configMapperClass',\n  'com.databricks.dbsql.config.SqlConfigMapperBridge'),\n ('spark.driver.maxResultSize', '4g'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'),\n ('spark.hadoop.fs.fcfs-s3.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.delta.multiClusterWrites.enabled', 'true'),\n ('spark.worker.cleanup.enabled', 'false'),\n ('spark.sql.legacy.createHiveTableByDefault', 'false'),\n ('spark.ui.port', '40001'),\n ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.attempts.maximum', '10'),\n ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType',\n  'ebs_volume_type: GENERAL_PURPOSE_SSD\\n'),\n ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'),\n ('spark.hadoop.fs.azure.user.agent.prefix', ''),\n ('spark.hadoop.fs.s3n.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough',\n  'false'),\n ('spark.hadoop.fs.fcfs-s3n.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'),\n ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDestination', ''),\n ('spark.databricks.wsfsPublicPreview', 'true'),\n ('spark.cleaner.referenceTracking.blocking', 'false'),\n ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'false'),\n ('spark.databricks.clusterUsageTags.clusterState', 'Pending'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes',\n  'false'),\n ('spark.databricks.tahoe.logStore.azure.class',\n  'com.databricks.tahoe.store.AzureLogStore'),\n ('spark.hadoop.fs.azure.skip.metrics', 'true'),\n ('spark.hadoop.hive.hmshandler.retry.attempts', '10'),\n ('spark.scheduler.mode', 'FAIR'),\n ('spark.sql.sources.default', 'delta'),\n ('spark.driver.port', '33265'),\n ('spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled',\n  'true'),\n ('spark.databricks.clusterUsageTags.clusterWorkers', '0'),\n ('spark.hadoop.fs.cpfs-s3n.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.cpfs-adl.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-abfss.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.rocksDB.fileManager.useCommitService', 'false'),\n ('spark.databricks.passthrough.oauth.refresher.impl',\n  'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'),\n ('spark.sql.hive.metastore.sharedPrefixes',\n  'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'),\n ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'),\n ('spark.task.reaper.killTimeout', '60s'),\n ('spark.hadoop.parquet.block.size.row.check.min', '10'),\n ('spark.hadoop.hive.server2.use.SSL', 'true'),\n ('spark.hadoop.fs.mcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'),\n ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '567751950135241'),\n ('spark.hadoop.hive.server2.keystore.path',\n  '/databricks/keys/jetty-ssl-driver-keystore.jks'),\n ('spark.databricks.credential.redactor',\n  'com.databricks.logging.secrets.CredentialRedactorProxyImpl'),\n ('spark.databricks.clusterUsageTags.clusterPinned', 'false'),\n ('spark.databricks.acl.provider',\n  'com.databricks.sql.acl.ReflectionBackedAclProvider'),\n ('spark.extraListeners',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled',\n  'false'),\n ('spark.sql.parquet.cacheMetadata', 'true'),\n ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'),\n ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'),\n ('spark.hadoop.fs.adl.impl', 'com.databricks.adl.AdlFileSystem'),\n ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'),\n ('spark.databricks.tahoe.logStore.class',\n  'com.databricks.tahoe.store.DelegatingLogStore'),\n ('spark.hadoop.fs.s3.impl.disable.cache', 'true'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins', '30'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins', '30'),\n ('libraryDownload.sleepIntervalSeconds', '5'),\n ('spark.databricks.cloudProvider', 'AWS'),\n ('spark.sql.hive.convertMetastoreParquet', 'true'),\n ('spark.executor.id', 'driver'),\n ('spark.databricks.service.dbutils.server.backend',\n  'com.databricks.dbconnect.SparkServerDBUtils'),\n ('spark.databricks.clusterUsageTags.driverContainerId',\n  'def70a50ef8a42d9a595acdfddc82052'),\n ('spark.databricks.clusterUsageTags.workerEnvironmentId',\n  'default-worker-env'),\n ('spark.databricks.repl.enableClassFileCleanup', 'true'),\n ('spark.databricks.clusterUsageTags.clusterName', 'My Cluster'),\n ('spark.hadoop.fs.s3a.multipart.size', '10485760'),\n ('spark.databricks.clusterUsageTags.cloudProvider', 'AWS'),\n ('spark.databricks.clusterUsageTags.effectiveSparkVersion',\n  '11.3.x-scala2.12'),\n ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'),\n ('spark.akka.frameSize', '256'),\n ('spark.app.startTime', '1668510383601'),\n ('spark.hadoop.fs.s3a.fast.upload', 'true'),\n ('spark.hadoop.fs.wasbs.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.sql.streaming.stopTimeout', '15s'),\n ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'),\n ('spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting',\n  'false'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'),\n ('spark.databricks.overrideDefaultCommitProtocol',\n  'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'),\n ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'),\n ('spark.databricks.clusterUsageTags.driverContainerPrivateIp',\n  '10.172.248.89'),\n ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'),\n ('libraryDownload.timeoutSeconds', '180'),\n ('spark.hadoop.parquet.memory.pool.ratio', '0.5'),\n ('spark.databricks.clusterUsageTags.clusterScalingType', 'fixed_size'),\n ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'),\n ('spark.hadoop.fs.s3a.block.size', '67108864'),\n ('spark.databricks.tahoe.logStore.gcp.class',\n  'com.databricks.tahoe.store.GCPLogStore'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.databricks.clusterUsageTags.sparkMasterUrlType', 'None'),\n ('spark.sql.sources.commitProtocolClass',\n  'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'),\n ('spark.hadoop.fs.fcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''),\n ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'),\n ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'),\n ('spark.shuffle.service.enabled', 'true'),\n ('spark.hadoop.fs.file.impl',\n  'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'),\n ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-s3.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''),\n ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'),\n ('spark.rpc.message.maxSize', '256'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''),\n ('spark.hadoop.fs.fcfs-wasbs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.driverNfs.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType',\n  'RDS_DIRECT'),\n ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'),\n ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.instanceProfileUsed', 'false'),\n ('spark.databricks.clusterUsageTags.userId', '567751950135241'),\n ('spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName',\n  'com.databricks.unity.TokenServiceApiTokenProvider'),\n ('spark.databricks.passthrough.glue.executorServiceFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'),\n ('spark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus',\n  'false'),\n ('spark.hadoop.fs.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemHadoop3'),\n ('spark.databricks.acl.scim.client',\n  'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'),\n ('spark.r.sql.derby.temp.dir', '/tmp/RtmpDpIYou'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'),\n ('spark.hadoop.fs.adl.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.block.size.row.check.max', '10'),\n ('spark.hadoop.fs.s3a.connection.maximum', '200'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'),\n ('spark.hadoop.fs.s3a.assumed.role.credentials.provider',\n  'com.amazonaws.auth.InstanceProfileCredentialsProvider'),\n ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'),\n ('spark.shuffle.reduceLocality.enabled', 'false'),\n ('spark.repl.class.uri', 'spark://10.172.248.89:33265/classes'),\n ('spark.databricks.clusterUsageTags.driverNodeType', 'dev-tier-node'),\n ('spark.hadoop.spark.sql.sources.outputCommitterClass',\n  'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'),\n ('spark.hadoop.fs.fcfs-abfs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.instanceBootstrapType', 'ssh'),\n ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.size', '1000'),\n ('spark.databricks.driverNodeTypeId', 'dev-tier-node'),\n ('spark.sql.parquet.compression.codec', 'snappy'),\n ('spark.hadoop.fs.stage.impl',\n  'com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem'),\n ('spark.databricks.credential.scope.fs.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider'),\n ('spark.databricks.cloudfetch.hasRegionSupport', 'true'),\n ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories',\n  'false'),\n ('spark.hadoop.fs.wasb.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.hadoop.fs.s3a.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.databricks.clusterUsageTags.sparkVersion', '11.3.x-scala2.12'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.size', '1000'),\n ('spark.app.id', 'local-1668510389809'),\n ('spark.databricks.workerNodeTypeId', 'dev-tier-node'),\n ('spark.hadoop.fs.abfs.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemHadoop3'),\n ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeSize', '0'),\n ('spark.sparklyr-backend.threads', '1'),\n ('spark.hadoop.fs.fcfs-wasb.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.passthrough.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'),\n ('spark.databricks.session.share', 'false'),\n ('spark.repl.class.outputDir',\n  '/local_disk0/tmp/repl/spark-8953665437868112782-ade1b467-b512-42da-81b4-54a978cafa76'),\n ('spark.databricks.clusterUsageTags.clusterResourceClass', 'default'),\n ('spark.hadoop.fs.s3n.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'),\n ('spark.driver.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n ('spark.hadoop.fs.dbfs.impl',\n  'com.databricks.backend.daemon.data.client.DbfsHadoop3'),\n ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'),\n ('spark.hadoop.fs.gs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode', 'CUSTOM'),\n ('spark.delta.sharing.profile.provider.class',\n  'io.delta.sharing.DeltaSharingCredentialsProvider'),\n ('spark.executor.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1'),\n ('spark.databricks.clusterUsageTags.clusterId', '1115-110204-6b1hu6qa'),\n ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeType',\n  'GENERAL_PURPOSE_SSD'),\n ('spark.databricks.automl.serviceEnabled', 'true'),\n ('spark.hadoop.parquet.page.size.check.estimate', 'false'),\n ('spark.databricks.clusterUsageTags.attribute_tag_service', ''),\n ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class',\n  'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'),\n ('spark.databricks.delta.preview.enabled', 'true'),\n ('spark.databricks.metrics.filesystem_io_metrics', 'true'),\n ('spark.databricks.cloudfetch.requesterClassName',\n  'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'),\n ('spark.master', 'local[8]'),\n ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'),\n ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'),\n ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeCount', '0'),\n ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'),\n ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'),\n ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'),\n ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'),\n ('spark.databricks.tahoe.logStore.aws.class',\n  'com.databricks.tahoe.store.S3LockBasedLogStore'),\n ('spark.speculation.quantile', '0.9'),\n ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'),\n ('spark.shuffle.manager', 'SORT'),\n ('spark.files.overwrite', 'true'),\n ('spark.databricks.credential.aws.secretKey.redactor',\n  'com.databricks.spark.util.AWSSecretKeyRedactorProxy'),\n ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '0'),\n ('spark.hadoop.fs.s3a.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes',\n  'false'),\n ('spark.r.numRBackendThreads', '1'),\n ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.azure.cache.invalidator.type',\n  'com.databricks.encryption.utils.CacheInvalidatorImpl'),\n ('spark.sql.hive.metastore.version', '0.13.0'),\n ('spark.shuffle.service.port', '4048'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'),\n ('spark.databricks.acl.client',\n  'com.databricks.spark.sql.acl.client.SparkSqlAclClient'),\n ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'),\n ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'),\n ('spark.databricks.clusterUsageTags.isServicePrincipalCluster', 'false'),\n ('spark.databricks.credential.scope.fs.impl',\n  'com.databricks.sql.acl.fs.CredentialScopeFileSystem'),\n ('spark.databricks.clusterUsageTags.enableElasticDisk', 'false'),\n ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'),\n ('spark.databricks.sparkContextId', '8953665437868112782'),\n ('spark.databricks.clusterUsageTags.clusterNodeType', 'dev-tier-node'),\n ('spark.databricks.passthrough.adls.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'),\n ('spark.app.name', 'Databricks Shell'),\n ('spark.driver.allowMultipleContexts', 'false'),\n ('spark.databricks.clusterUsageTags.driverInstancePrivateIp',\n  '10.172.242.61'),\n ('spark.hadoop.fs.AbstractFileSystem.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'),\n ('spark.databricks.secret.sparkConf.keys.toRedact', ''),\n ('spark.rdd.compress', 'true'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException',\n  'false'),\n ('spark.databricks.python.defaultPythonRepl', 'ipykernel'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''),\n ('spark.databricks.eventLog.dir', 'eventlogs'),\n ('spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider'),\n ('spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled', 'false'),\n ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'),\n ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'),\n ('spark.speculation', 'false'),\n ('spark.hadoop.databricks.dbfs.client.version', 'v1'),\n ('spark.databricks.clusterUsageTags.orgId', '3608311878953391'),\n ('spark.hadoop.hive.server2.session.check.interval', '60000'),\n ('spark.sql.hive.convertCTAS', 'true'),\n ('spark.hadoop.fs.s3a.max.total.tasks', '1000'),\n ('spark.hadoop.spark.sql.parquet.output.committer.class',\n  'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'),\n ('spark.hadoop.fs.s3a.fast.upload.default', 'true'),\n ('spark.databricks.clusterUsageTags.clusterGeneration', '0'),\n ('spark.hadoop.fs.mlflowdbfs.impl',\n  'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'),\n ('spark.databricks.eventLog.listenerClassName',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'),\n ('spark.speculation.multiplier', '3'),\n ('spark.storage.blockManagerTimeoutIntervalMs', '300000'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvId',\n  'default-worker-env'),\n ('spark.sparkr.use.daemon', 'false'),\n ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '3608311878953391'),\n ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'),\n ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'),\n ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'),\n ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'),\n ('spark.hadoop.fs.s3a.threads.max', '136'),\n ('spark.r.backendConnectionTimeout', '604800'),\n ('spark.hadoop.fs.abfss.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystemHadoop3'),\n ('spark.driver.host', '10.172.248.89'),\n ('spark.hadoop.hive.server2.idle.session.timeout', '900000'),\n ('spark.databricks.redactor',\n  'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'),\n ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '120'),\n ('spark.executor.extraClassPath',\n  '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*'),\n ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'us-west-2'),\n ('spark.logConf', 'true'),\n ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'),\n ('spark.hadoop.hive.server2.enable.doAs', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedSparkVersion',\n  '11.3.x-scala2.12'),\n ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'),\n ('spark.hadoop.spark.driverproxy.customHeadersToProperties',\n  'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name'),\n ('spark.shuffle.memoryFraction', '0.2'),\n ('spark.databricks.clusterUsageTags.clusterAllTags',\n  '[{\"key\":\"Name\",\"value\":\"ce-worker\"}]'),\n ('spark.hadoop.fs.dbfsartifacts.impl',\n  'com.databricks.backend.daemon.data.client.DBFSV1'),\n ('spark.hadoop.fs.cpfs-s3a.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.containerZoneId', 'us-west-2c'),\n ('spark.hadoop.fs.s3a.connection.timeout', '50000'),\n ('spark.databricks.secret.envVar.keys.toRedact', ''),\n ('spark.databricks.clusterUsageTags.region', 'us-west-2'),\n ('spark.databricks.clusterUsageTags.clusterSpotBidPricePercent', '100'),\n ('spark.files.useFetchCache', 'false')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[29]: [('spark.databricks.preemption.enabled', 'true'),\n ('spark.sql.hive.metastore.jars', '/databricks/databricks-hive/*'),\n ('spark.driver.tempDirectory', '/local_disk0/tmp'),\n ('spark.sql.warehouse.dir', 'dbfs:/user/hive/warehouse'),\n ('spark.databricks.managedCatalog.clientClassName',\n  'com.databricks.managedcatalog.ManagedCatalogClientImpl'),\n ('spark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeGCPTokenProvider'),\n ('spark.hadoop.fs.fcfs-s3.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.retry.limit', '20'),\n ('spark.sql.streaming.checkpointFileManagerClass',\n  'com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager'),\n ('spark.databricks.service.dbutils.repl.backend',\n  'com.databricks.dbconnect.ReplDBUtils'),\n ('spark.databricks.clusterUsageTags.driverPublicDns',\n  'ec2-54-214-224-53.us-west-2.compute.amazonaws.com'),\n ('spark.hadoop.databricks.s3.verifyBucketExists.enabled', 'false'),\n ('spark.streaming.driver.writeAheadLog.allowBatching', 'true'),\n ('spark.databricks.clusterSource', 'UI'),\n ('spark.hadoop.hive.server2.transport.mode', 'http'),\n ('spark.executor.memory', '8278m'),\n ('spark.databricks.clusterUsageTags.driverInstanceId', 'i-0207c768ae41e4ff3'),\n ('spark.hadoop.fs.cpfs-adl.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.hailEnabled', 'false'),\n ('spark.hadoop.databricks.s3.amazonS3Client.cache.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled', 'false'),\n ('spark.databricks.clusterUsageTags.containerType', 'LXC'),\n ('spark.eventLog.enabled', 'false'),\n ('spark.databricks.clusterUsageTags.isIMv2Enabled', 'false'),\n ('spark.hadoop.fs.stage.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.hmshandler.retry.interval', '2000'),\n ('spark.executor.tempDirectory', '/local_disk0/tmp'),\n ('spark.hadoop.fs.azure.authorization.caching.enable', 'false'),\n ('spark.hadoop.fs.fcfs-abfss.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.mapred.output.committer.class',\n  'com.databricks.backend.daemon.data.client.DirectOutputCommitter'),\n ('spark.hadoop.hive.server2.thrift.http.port', '10000'),\n ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version', '2'),\n ('spark.sql.allowMultipleContexts', 'false'),\n ('spark.databricks.eventLog.enabled', 'true'),\n ('spark.home', '/databricks/spark'),\n ('spark.databricks.clusterUsageTags.clusterTargetWorkers', '0'),\n ('spark.hadoop.hive.server2.idle.operation.timeout', '7200000'),\n ('spark.task.reaper.enabled', 'true'),\n ('spark.storage.memoryFraction', '0.5'),\n ('eventLog.rolloverIntervalSeconds', '900'),\n ('spark.databricks.clusterUsageTags.clusterFirstOnDemand', '0'),\n ('spark.databricks.sql.configMapperClass',\n  'com.databricks.dbsql.config.SqlConfigMapperBridge'),\n ('spark.driver.maxResultSize', '4g'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsNewline', 'false'),\n ('spark.hadoop.fs.fcfs-s3.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.delta.multiClusterWrites.enabled', 'true'),\n ('spark.worker.cleanup.enabled', 'false'),\n ('spark.sql.legacy.createHiveTableByDefault', 'false'),\n ('spark.ui.port', '40001'),\n ('spark.hadoop.fs.fcfs-s3a.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3a.attempts.maximum', '10'),\n ('spark.databricks.clusterUsageTags.enableCredentialPassthrough', 'false'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType',\n  'ebs_volume_type: GENERAL_PURPOSE_SSD\\n'),\n ('spark.databricks.clusterUsageTags.enableJdbcAutoStart', 'true'),\n ('spark.hadoop.fs.azure.user.agent.prefix', ''),\n ('spark.hadoop.fs.s3n.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough',\n  'false'),\n ('spark.hadoop.fs.fcfs-s3n.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.hadoop.fs.s3a.retry.throttle.interval', '500ms'),\n ('spark.hadoop.fs.wasb.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.clusterLogDestination', ''),\n ('spark.databricks.wsfsPublicPreview', 'true'),\n ('spark.cleaner.referenceTracking.blocking', 'false'),\n ('spark.databricks.clusterUsageTags.isSingleUserCluster', 'false'),\n ('spark.databricks.clusterUsageTags.clusterState', 'Pending'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes',\n  'false'),\n ('spark.databricks.tahoe.logStore.azure.class',\n  'com.databricks.tahoe.store.AzureLogStore'),\n ('spark.hadoop.fs.azure.skip.metrics', 'true'),\n ('spark.hadoop.hive.hmshandler.retry.attempts', '10'),\n ('spark.scheduler.mode', 'FAIR'),\n ('spark.sql.sources.default', 'delta'),\n ('spark.driver.port', '33265'),\n ('spark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled',\n  'true'),\n ('spark.databricks.clusterUsageTags.clusterWorkers', '0'),\n ('spark.hadoop.fs.cpfs-s3n.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.cpfs-adl.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.hadoop.fs.fcfs-s3n.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-abfss.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.rocksDB.fileManager.useCommitService', 'false'),\n ('spark.databricks.passthrough.oauth.refresher.impl',\n  'com.databricks.backend.daemon.driver.credentials.OAuthTokenRefresherClient'),\n ('spark.sql.hive.metastore.sharedPrefixes',\n  'org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks'),\n ('spark.databricks.io.directoryCommit.enableLogicalDelete', 'false'),\n ('spark.task.reaper.killTimeout', '60s'),\n ('spark.hadoop.parquet.block.size.row.check.min', '10'),\n ('spark.hadoop.hive.server2.use.SSL', 'true'),\n ('spark.hadoop.fs.mcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.ManagedCatalogFileSystem'),\n ('spark.databricks.clusterUsageTags.clusterAvailability', 'ON_DEMAND'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb', '0'),\n ('spark.databricks.clusterUsageTags.clusterOwnerUserId', '567751950135241'),\n ('spark.hadoop.hive.server2.keystore.path',\n  '/databricks/keys/jetty-ssl-driver-keystore.jks'),\n ('spark.databricks.credential.redactor',\n  'com.databricks.logging.secrets.CredentialRedactorProxyImpl'),\n ('spark.databricks.clusterUsageTags.clusterPinned', 'false'),\n ('spark.databricks.acl.provider',\n  'com.databricks.sql.acl.ReflectionBackedAclProvider'),\n ('spark.extraListeners',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled',\n  'false'),\n ('spark.sql.parquet.cacheMetadata', 'true'),\n ('spark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2', '0'),\n ('spark.hadoop.parquet.abfs.readahead.optimization.enabled', 'true'),\n ('spark.hadoop.fs.adl.impl', 'com.databricks.adl.AdlFileSystem'),\n ('spark.hadoop.fs.cpfs-abfss.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.enableLocalDiskEncryption', 'false'),\n ('spark.databricks.tahoe.logStore.class',\n  'com.databricks.tahoe.store.DelegatingLogStore'),\n ('spark.hadoop.fs.s3.impl.disable.cache', 'true'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins', '30'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins', '30'),\n ('libraryDownload.sleepIntervalSeconds', '5'),\n ('spark.databricks.cloudProvider', 'AWS'),\n ('spark.sql.hive.convertMetastoreParquet', 'true'),\n ('spark.executor.id', 'driver'),\n ('spark.databricks.service.dbutils.server.backend',\n  'com.databricks.dbconnect.SparkServerDBUtils'),\n ('spark.databricks.clusterUsageTags.driverContainerId',\n  'def70a50ef8a42d9a595acdfddc82052'),\n ('spark.databricks.clusterUsageTags.workerEnvironmentId',\n  'default-worker-env'),\n ('spark.databricks.repl.enableClassFileCleanup', 'true'),\n ('spark.databricks.clusterUsageTags.clusterName', 'My Cluster'),\n ('spark.hadoop.fs.s3a.multipart.size', '10485760'),\n ('spark.databricks.clusterUsageTags.cloudProvider', 'AWS'),\n ('spark.databricks.clusterUsageTags.effectiveSparkVersion',\n  '11.3.x-scala2.12'),\n ('spark.metrics.conf', '/databricks/spark/conf/metrics.properties'),\n ('spark.akka.frameSize', '256'),\n ('spark.app.startTime', '1668510383601'),\n ('spark.hadoop.fs.s3a.fast.upload', 'true'),\n ('spark.hadoop.fs.wasbs.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.sql.streaming.stopTimeout', '15s'),\n ('spark.hadoop.hive.server2.keystore.password', '[REDACTED]'),\n ('spark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting',\n  'false'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsEscape', 'false'),\n ('spark.databricks.overrideDefaultCommitProtocol',\n  'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol'),\n ('spark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient'),\n ('spark.databricks.clusterUsageTags.driverContainerPrivateIp',\n  '10.172.248.89'),\n ('spark.databricks.clusterUsageTags.clusterNoDriverDaemon', 'false'),\n ('libraryDownload.timeoutSeconds', '180'),\n ('spark.hadoop.parquet.memory.pool.ratio', '0.5'),\n ('spark.databricks.clusterUsageTags.clusterScalingType', 'fixed_size'),\n ('spark.databricks.passthrough.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlGen2CredentialContextTokenProvider'),\n ('spark.hadoop.fs.s3a.block.size', '67108864'),\n ('spark.databricks.tahoe.logStore.gcp.class',\n  'com.databricks.tahoe.store.GCPLogStore'),\n ('spark.serializer.objectStreamReset', '100'),\n ('spark.databricks.clusterUsageTags.sparkMasterUrlType', 'None'),\n ('spark.sql.sources.commitProtocolClass',\n  'com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol'),\n ('spark.hadoop.fs.fcfs-s3a.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_budget', ''),\n ('spark.databricks.clusterUsageTags.clusterPythonVersion', '3'),\n ('spark.databricks.clusterUsageTags.enableDfAcls', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount', '0'),\n ('spark.shuffle.service.enabled', 'true'),\n ('spark.hadoop.fs.file.impl',\n  'com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem'),\n ('spark.hadoop.fs.fcfs-wasb.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.cpfs-s3.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_maintainer', ''),\n ('spark.hadoop.fs.s3a.multipart.threshold', '104857600'),\n ('spark.rpc.message.maxSize', '256'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_suite', ''),\n ('spark.hadoop.fs.fcfs-wasbs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.driverNfs.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.clusterMetastoreAccessType',\n  'RDS_DIRECT'),\n ('spark.databricks.clusterUsageTags.ngrokNpipEnabled', 'false'),\n ('spark.hadoop.parquet.page.metadata.validation.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.instanceProfileUsed', 'false'),\n ('spark.databricks.clusterUsageTags.userId', '567751950135241'),\n ('spark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName',\n  'com.databricks.unity.TokenServiceApiTokenProvider'),\n ('spark.databricks.passthrough.glue.executorServiceFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.GlueClientExecutorServiceFactory'),\n ('spark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus',\n  'false'),\n ('spark.hadoop.fs.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemHadoop3'),\n ('spark.databricks.acl.scim.client',\n  'com.databricks.spark.sql.acl.client.DriverToWebappScimClient'),\n ('spark.r.sql.derby.temp.dir', '/tmp/RtmpDpIYou'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick', 'false'),\n ('spark.hadoop.fs.adl.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.block.size.row.check.max', '10'),\n ('spark.hadoop.fs.s3a.connection.maximum', '200'),\n ('spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2', '0'),\n ('spark.hadoop.fs.s3a.assumed.role.credentials.provider',\n  'com.amazonaws.auth.InstanceProfileCredentialsProvider'),\n ('spark.hadoop.fs.s3a.fast.upload.active.blocks', '32'),\n ('spark.shuffle.reduceLocality.enabled', 'false'),\n ('spark.repl.class.uri', 'spark://10.172.248.89:33265/classes'),\n ('spark.databricks.clusterUsageTags.driverNodeType', 'dev-tier-node'),\n ('spark.hadoop.spark.sql.sources.outputCommitterClass',\n  'com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter'),\n ('spark.hadoop.fs.fcfs-abfs.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.clusterUsageTags.instanceBootstrapType', 'ssh'),\n ('spark.hadoop.fs.fcfs-abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled', 'false'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.table.size', '1000'),\n ('spark.databricks.driverNodeTypeId', 'dev-tier-node'),\n ('spark.sql.parquet.compression.codec', 'snappy'),\n ('spark.hadoop.fs.stage.impl',\n  'com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem'),\n ('spark.databricks.credential.scope.fs.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider'),\n ('spark.databricks.cloudfetch.hasRegionSupport', 'true'),\n ('spark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories',\n  'false'),\n ('spark.hadoop.fs.wasb.impl',\n  'shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem'),\n ('spark.hadoop.fs.s3a.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.databricks.clusterUsageTags.sparkVersion', '11.3.x-scala2.12'),\n ('spark.hadoop.spark.hadoop.aws.glue.cache.db.size', '1000'),\n ('spark.app.id', 'local-1668510389809'),\n ('spark.databricks.workerNodeTypeId', 'dev-tier-node'),\n ('spark.hadoop.fs.abfs.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemHadoop3'),\n ('spark.databricks.passthrough.glue.credentialsProviderFactoryClassName',\n  'com.databricks.backend.daemon.driver.credentials.DatabricksCredentialProviderFactory'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeSize', '0'),\n ('spark.sparklyr-backend.threads', '1'),\n ('spark.hadoop.fs.fcfs-wasb.impl',\n  'com.databricks.sql.acl.fs.FixedCredentialsFileSystem'),\n ('spark.databricks.passthrough.s3a.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.aws.AwsCredentialContextTokenProvider'),\n ('spark.databricks.session.share', 'false'),\n ('spark.repl.class.outputDir',\n  '/local_disk0/tmp/repl/spark-8953665437868112782-ade1b467-b512-42da-81b4-54a978cafa76'),\n ('spark.databricks.clusterUsageTags.clusterResourceClass', 'default'),\n ('spark.hadoop.fs.s3n.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.hadoop.fs.idbfs.impl', 'com.databricks.io.idbfs.IdbfsFileSystem'),\n ('spark.driver.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n ('spark.hadoop.fs.dbfs.impl',\n  'com.databricks.backend.daemon.data.client.DbfsHadoop3'),\n ('spark.databricks.clusterUsageTags.clusterSku', 'STANDARD_SKU'),\n ('spark.hadoop.fs.gs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.s3.impl',\n  'shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystemHadoop3'),\n ('spark.databricks.clusterUsageTags.clusterUnityCatalogMode', 'CUSTOM'),\n ('spark.delta.sharing.profile.provider.class',\n  'io.delta.sharing.DeltaSharingCredentialsProvider'),\n ('spark.executor.extraJavaOptions',\n  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -XX:+PrintGCDetails -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1'),\n ('spark.databricks.clusterUsageTags.clusterId', '1115-110204-6b1hu6qa'),\n ('spark.worker.aioaLazyConfig.iamReadinessCheckClientClass',\n  'com.databricks.backend.daemon.driver.NephosIamRoleCheckClient'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeType',\n  'GENERAL_PURPOSE_SSD'),\n ('spark.databricks.automl.serviceEnabled', 'true'),\n ('spark.hadoop.parquet.page.size.check.estimate', 'false'),\n ('spark.databricks.clusterUsageTags.attribute_tag_service', ''),\n ('spark.databricks.passthrough.s3a.threadPoolExecutor.factory.class',\n  'com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory'),\n ('spark.databricks.delta.preview.enabled', 'true'),\n ('spark.databricks.metrics.filesystem_io_metrics', 'true'),\n ('spark.databricks.cloudfetch.requesterClassName',\n  'com.databricks.spark.sql.cloudfetch.DataDaemonCloudPresignedUrlRequester'),\n ('spark.master', 'local[8]'),\n ('spark.databricks.delta.logStore.crossCloud.fatal', 'true'),\n ('spark.databricks.driverNfs.clusterWidePythonLibsEnabled', 'true'),\n ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'),\n ('spark.databricks.clusterUsageTags.enableSqlAclsOnly', 'false'),\n ('spark.databricks.clusterUsageTags.clusterEbsVolumeCount', '0'),\n ('spark.databricks.clusterUsageTags.clusterSizeType', 'VM_CONTAINER'),\n ('spark.hadoop.databricks.fs.perfMetrics.enable', 'true'),\n ('spark.databricks.clusterUsageTags.clusterNumSshKeys', '0'),\n ('spark.hadoop.fs.gs.outputstream.upload.chunk.size', '16777216'),\n ('spark.databricks.tahoe.logStore.aws.class',\n  'com.databricks.tahoe.store.S3LockBasedLogStore'),\n ('spark.speculation.quantile', '0.9'),\n ('spark.databricks.clusterUsageTags.privateLinkEnabled', 'false'),\n ('spark.shuffle.manager', 'SORT'),\n ('spark.files.overwrite', 'true'),\n ('spark.databricks.credential.aws.secretKey.redactor',\n  'com.databricks.spark.util.AWSSecretKeyRedactorProxy'),\n ('spark.databricks.clusterUsageTags.clusterNumCustomTags', '0'),\n ('spark.hadoop.fs.s3a.impl.disable.cache', 'true'),\n ('spark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes',\n  'false'),\n ('spark.r.numRBackendThreads', '1'),\n ('spark.hadoop.fs.wasbs.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.abfss.impl.disable.cache', 'true'),\n ('spark.hadoop.fs.azure.cache.invalidator.type',\n  'com.databricks.encryption.utils.CacheInvalidatorImpl'),\n ('spark.sql.hive.metastore.version', '0.13.0'),\n ('spark.shuffle.service.port', '4048'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType', 'default'),\n ('spark.databricks.acl.client',\n  'com.databricks.spark.sql.acl.client.SparkSqlAclClient'),\n ('spark.streaming.driver.writeAheadLog.closeFileAfterWrite', 'true'),\n ('spark.hadoop.hive.warehouse.subdir.inherit.perms', 'false'),\n ('spark.databricks.clusterUsageTags.isServicePrincipalCluster', 'false'),\n ('spark.databricks.credential.scope.fs.impl',\n  'com.databricks.sql.acl.fs.CredentialScopeFileSystem'),\n ('spark.databricks.clusterUsageTags.enableElasticDisk', 'false'),\n ('spark.hadoop.fs.fcfs-wasbs.impl.disable.cache', 'true'),\n ('spark.databricks.sparkContextId', '8953665437868112782'),\n ('spark.databricks.clusterUsageTags.clusterNodeType', 'dev-tier-node'),\n ('spark.databricks.passthrough.adls.tokenProviderClassName',\n  'com.databricks.backend.daemon.data.client.adl.AdlCredentialContextTokenProvider'),\n ('spark.app.name', 'Databricks Shell'),\n ('spark.driver.allowMultipleContexts', 'false'),\n ('spark.databricks.clusterUsageTags.driverInstancePrivateIp',\n  '10.172.242.61'),\n ('spark.hadoop.fs.AbstractFileSystem.gs.impl',\n  'shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS'),\n ('spark.databricks.secret.sparkConf.keys.toRedact', ''),\n ('spark.rdd.compress', 'true'),\n ('spark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException',\n  'false'),\n ('spark.databricks.python.defaultPythonRepl', 'ipykernel'),\n ('spark.databricks.clusterUsageTags.attribute_tag_dust_execution_env', ''),\n ('spark.databricks.eventLog.dir', 'eventlogs'),\n ('spark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName',\n  'com.databricks.backend.daemon.driver.credentials.CredentialScopeADLSTokenProvider'),\n ('spark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled', 'false'),\n ('spark.databricks.driverNfs.pathSuffix', '.ephemeral_nfs'),\n ('spark.databricks.clusterUsageTags.clusterCreator', 'Webapp'),\n ('spark.speculation', 'false'),\n ('spark.hadoop.databricks.dbfs.client.version', 'v1'),\n ('spark.databricks.clusterUsageTags.orgId', '3608311878953391'),\n ('spark.hadoop.hive.server2.session.check.interval', '60000'),\n ('spark.sql.hive.convertCTAS', 'true'),\n ('spark.hadoop.fs.s3a.max.total.tasks', '1000'),\n ('spark.hadoop.spark.sql.parquet.output.committer.class',\n  'org.apache.spark.sql.parquet.DirectParquetOutputCommitter'),\n ('spark.hadoop.fs.s3a.fast.upload.default', 'true'),\n ('spark.databricks.clusterUsageTags.clusterGeneration', '0'),\n ('spark.hadoop.fs.mlflowdbfs.impl',\n  'com.databricks.mlflowdbfs.MlflowdbfsFileSystem'),\n ('spark.databricks.eventLog.listenerClassName',\n  'com.databricks.backend.daemon.driver.DBCEventLoggingListener'),\n ('spark.hadoop.fs.abfs.impl.disable.cache', 'true'),\n ('spark.speculation.multiplier', '3'),\n ('spark.storage.blockManagerTimeoutIntervalMs', '300000'),\n ('spark.databricks.clusterUsageTags.instanceWorkerEnvId',\n  'default-worker-env'),\n ('spark.sparkr.use.daemon', 'false'),\n ('spark.databricks.clusterUsageTags.clusterOwnerOrgId', '3608311878953391'),\n ('spark.scheduler.listenerbus.eventqueue.capacity', '20000'),\n ('spark.databricks.clusterUsageTags.clusterStateMessage', 'Starting Spark'),\n ('spark.hadoop.parquet.page.write-checksum.enabled', 'true'),\n ('spark.hadoop.databricks.s3commit.client.sslTrustAll', 'false'),\n ('spark.hadoop.fs.s3a.threads.max', '136'),\n ('spark.r.backendConnectionTimeout', '604800'),\n ('spark.hadoop.fs.abfss.impl',\n  'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystemHadoop3'),\n ('spark.driver.host', '10.172.248.89'),\n ('spark.hadoop.hive.server2.idle.session.timeout', '900000'),\n ('spark.databricks.redactor',\n  'com.databricks.spark.util.DatabricksSparkLogRedactorProxy'),\n ('spark.databricks.clusterUsageTags.autoTerminationMinutes', '120'),\n ('spark.executor.extraClassPath',\n  '/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*'),\n ('spark.hadoop.fs.fcfs-abfs.impl.disable.cache', 'true'),\n ('spark.hadoop.parquet.page.verify-checksum.enabled', 'true'),\n ('spark.databricks.clusterUsageTags.dataPlaneRegion', 'us-west-2'),\n ('spark.logConf', 'true'),\n ('spark.databricks.clusterUsageTags.enableJobsAutostart', 'true'),\n ('spark.hadoop.hive.server2.enable.doAs', 'false'),\n ('spark.databricks.clusterUsageTags.userProvidedSparkVersion',\n  '11.3.x-scala2.12'),\n ('spark.hadoop.parquet.filter.columnindex.enabled', 'false'),\n ('spark.hadoop.spark.driverproxy.customHeadersToProperties',\n  'X-Databricks-User-Token:spark.databricks.token,X-Databricks-Non-UC-User-Token:spark.databricks.non.uc.token,X-Databricks-Api-Url:spark.databricks.api.url,X-Databricks-ADLS-Gen1-Token:spark.databricks.adls.gen1.token,X-Databricks-ADLS-Gen2-Token:spark.databricks.adls.gen2.token,X-Databricks-Synapse-Token:spark.databricks.synapse.token,X-Databricks-AWS-Credentials:spark.databricks.aws.creds,X-Databricks-User-Id:spark.databricks.user.id,X-Databricks-User-Name:spark.databricks.user.name'),\n ('spark.shuffle.memoryFraction', '0.2'),\n ('spark.databricks.clusterUsageTags.clusterAllTags',\n  '[{\"key\":\"Name\",\"value\":\"ce-worker\"}]'),\n ('spark.hadoop.fs.dbfsartifacts.impl',\n  'com.databricks.backend.daemon.data.client.DBFSV1'),\n ('spark.hadoop.fs.cpfs-s3a.impl',\n  'com.databricks.sql.acl.fs.CredentialPassthroughFileSystem'),\n ('spark.databricks.clusterUsageTags.containerZoneId', 'us-west-2c'),\n ('spark.hadoop.fs.s3a.connection.timeout', '50000'),\n ('spark.databricks.secret.envVar.keys.toRedact', ''),\n ('spark.databricks.clusterUsageTags.region', 'us-west-2'),\n ('spark.databricks.clusterUsageTags.clusterSpotBidPricePercent', '100'),\n ('spark.files.useFetchCache', 'false')]"]}}],"execution_count":0},{"cell_type":"code","source":["sc.version"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5cbe82cb-8b34-4d41-a3f5-48164dd5e35c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[31]: '3.3.0'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[31]: '3.3.0'"]}}],"execution_count":0},{"cell_type":"code","source":["#rdd from csv file\n#df=spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/FileStore/tables/test1-1.csv\")\nnew_rdd=sc.textFile(\"/FileStore/tables/test2.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45560f64-0642-4ab6-8173-5365b89e96aa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(new_rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"541088dc-cfe9-40db-8cd9-9c4196662321","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[70]: pyspark.rdd.RDD","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[70]: pyspark.rdd.RDD"]}}],"execution_count":0},{"cell_type":"code","source":["print(new_rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7fea4249-89aa-4fc9-87b5-b86a8d42d866","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/FileStore/tables/test2.csv MapPartitionsRDD[73] at textFile at NativeMethodAccessorImpl.java:0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/FileStore/tables/test2.csv MapPartitionsRDD[73] at textFile at NativeMethodAccessorImpl.java:0\n"]}}],"execution_count":0},{"cell_type":"code","source":["type(new_rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90e14b4c-a05b-4fa1-a4b2-7ef8bbee5845","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[73]: pyspark.rdd.RDD","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[73]: pyspark.rdd.RDD"]}}],"execution_count":0},{"cell_type":"code","source":["new_rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"856ff02e-7b68-4fd8-9a9e-7978d54145be","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-4267184453141779>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnew_rdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1217\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1218\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1219\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1220\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 124) (ip-10-172-248-89.us-west-2.compute.internal executor driver): java.lang.NullPointerException\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$4(DbfsHadoop3.scala:52)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:52)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:296)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:295)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:253)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:372)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:372)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:168)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:136)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3271)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3203)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3194)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3194)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1421)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3483)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3409)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1166)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1154)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2706)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1027)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1025)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:282)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$4(DbfsHadoop3.scala:52)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:52)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:296)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:295)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:253)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:372)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:372)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:168)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:136)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 124) (ip-10-172-248-89.us-west-2.compute.internal executor driver): java.lang.NullPointerException","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-4267184453141779>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mnew_rdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1217\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1218\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1219\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1220\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 124) (ip-10-172-248-89.us-west-2.compute.internal executor driver): java.lang.NullPointerException\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$4(DbfsHadoop3.scala:52)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:52)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:296)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:295)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:253)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:372)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:372)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:168)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:136)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3271)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3203)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3194)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3194)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1421)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3483)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3409)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1166)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1154)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2706)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1027)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1025)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:282)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$4(DbfsHadoop3.scala:52)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:52)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768)\n\tat org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115)\n\tat org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:296)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:295)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:253)\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:372)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:372)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:336)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:168)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:136)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]}}],"execution_count":0},{"cell_type":"code","source":["df=spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/FileStore/tables/test1-1.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ba4433ff-cbf8-456e-ad34-4d4833e9de00","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f8c7d67f-a566-49dd-8858-d30f80528e73","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---------+---+----------+------+\n|     Name|age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["#df to rdd\nrdd=df.rdd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e5e5e85-c70a-48e2-acbb-2e18206852cc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60b02a84-2dba-4999-b89f-0ed96229ec62","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"MapPartitionsRDD[64] at javaToPython at NativeMethodAccessorImpl.java:0\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["MapPartitionsRDD[64] at javaToPython at NativeMethodAccessorImpl.java:0\n"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"348c32bf-97de-4319-9d9b-3c1bb12235a0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[60]: [Row(name='James', salary=3000),\n Row(name='Anna', salary=4001),\n Row(name='Robert', salary=6200)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[60]: [Row(name='James', salary=3000),\n Row(name='Anna', salary=4001),\n Row(name='Robert', salary=6200)]"]}}],"execution_count":0},{"cell_type":"code","source":["print(rdd.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2328dde2-dfd7-435d-8ca4-2814a6c135dd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[Row(name='James', salary=3000), Row(name='Anna', salary=4001), Row(name='Robert', salary=6200)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[Row(name='James', salary=3000), Row(name='Anna', salary=4001), Row(name='Robert', salary=6200)]\n"]}}],"execution_count":0},{"cell_type":"code","source":["type(rdd)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d6c9ae7f-e459-4bda-86a8-2851bfa5557f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[62]: pyspark.rdd.RDD","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[62]: pyspark.rdd.RDD"]}}],"execution_count":0},{"cell_type":"code","source":["#rdd to DF\ndf1=rdd.toDF()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6844228f-8fa2-4838-a2c4-93fa66d2ddbc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["type(df1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"233c1006-d9e7-43c5-ba65-a142dde3db42","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[67]: pyspark.sql.dataframe.DataFrame","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[67]: pyspark.sql.dataframe.DataFrame"]}}],"execution_count":0},{"cell_type":"code","source":["df1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5fe5afa-6779-43f7-af61-1b67f33bf6c3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+------+\n|  name|salary|\n+------+------+\n| James|  3000|\n|  Anna|  4001|\n|Robert|  6200|\n+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+------+\n|  name|salary|\n+------+------+\n| James|  3000|\n|  Anna|  4001|\n|Robert|  6200|\n+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9d2533ae-5221-4cbd-9f4a-1f8452e596f3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+------+------+\n|  name|salary|\n+------+------+\n| James|  3000|\n|  Anna|  4001|\n|Robert|  6200|\n+------+------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+------+------+\n|  name|salary|\n+------+------+\n| James|  3000|\n|  Anna|  4001|\n|Robert|  6200|\n+------+------+\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["# Reduce: sum of all values\nrdd_1=sc.parallelize([1,2,3,4,5,6,7,8,9,10])\nrdd_1.reduce(lambda a, b:a+b)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d1808f8e-0635-4566-b02f-715c8090459a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[79]: 55","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[79]: 55"]}}],"execution_count":0},{"cell_type":"code","source":["#distinct values\nrdd_2=sc.parallelize([1,2,3,4,5,6,7,8,9,10,1,2,4,2,5])\nrdd_3=rdd_2.distinct()\nprint(rdd_3.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfeeaea9-3680-47cd-bc22-d564370f85c6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[8, 1, 9, 2, 10, 3, 4, 5, 6, 7]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[8, 1, 9, 2, 10, 3, 4, 5, 6, 7]\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35e20156-e1fc-40d7-bc83-913e88d5cfa5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[8, 1, 9, 2, 10, 3, 4, 5, 6, 7]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[8, 1, 9, 2, 10, 3, 4, 5, 6, 7]\n"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f82fb0f-e1cd-40cb-920b-21f738b04753","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[83]: [Row(name='James', salary=3000),\n Row(name='Anna', salary=4001),\n Row(name='Robert', salary=6200)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[83]: [Row(name='James', salary=3000),\n Row(name='Anna', salary=4001),\n Row(name='Robert', salary=6200)]"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.take(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ba825c6c-d7d2-4c64-ba92-2fd8ec79f4c2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[84]: [Row(name='James', salary=3000)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[84]: [Row(name='James', salary=3000)]"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.top(1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11a55a25-184a-4fb0-ab80-30e2b20da2b6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[85]: [Row(name='Robert', salary=6200)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[85]: [Row(name='Robert', salary=6200)]"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8445f828-36ec-4370-b932-5ba64ec690f8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[86]: 3","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[86]: 3"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.min()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"92209dae-9530-4842-bd5a-9fa654fa4d7c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[87]: Row(name='Anna', salary=4001)","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[87]: Row(name='Anna', salary=4001)"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.max()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11c5c8a7-c272-4443-97e6-930f599464d9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[88]: Row(name='Robert', salary=6200)","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[88]: Row(name='Robert', salary=6200)"]}}],"execution_count":0},{"cell_type":"code","source":["rdd.mean()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6adedf5a-c360-473c-8e16-b2187412a704","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-575433596223570>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mmean\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1724\u001B[0m         \u001B[0;36m2.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1725\u001B[0m         \"\"\"\n\u001B[0;32m-> 1726\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstats\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[return-value]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1727\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1728\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mvariance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"RDD[NumberOrArray]\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"NumberOrArray\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mstats\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1572\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mleft_counter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmergeStats\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mright_counter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1574\u001B[0;31m         return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(  # type: ignore[arg-type]\n\u001B[0m\u001B[1;32m   1575\u001B[0m             \u001B[0mredFunc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1576\u001B[0m         )\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mreduce\u001B[0;34m(self, f)\u001B[0m\n\u001B[1;32m   1290\u001B[0m             \u001B[0;32myield\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minitial\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1291\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1292\u001B[0;31m         \u001B[0mvals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1293\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1294\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1217\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1218\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1219\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1220\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 49.0 failed 1 times, most recent failure: Lost task 2.0 in stage 49.0 (TID 246) (ip-10-172-248-89.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: unsupported operand type(s) for -: 'Row' and 'float''. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1019, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1009, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 3517, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 543, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 1574, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(  # type: ignore[arg-type]\n  File \"/databricks/spark/python/pyspark/statcounter.py\", line 43, in __init__\n    self.merge(v)\n  File \"/databricks/spark/python/pyspark/statcounter.py\", line 47, in merge\n    delta = value - self.mu\nTypeError: unsupported operand type(s) for -: 'Row' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:904)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1029)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:168)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:136)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3271)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3203)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3194)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3194)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1421)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3483)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3409)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1166)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1154)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2706)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1027)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1025)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:282)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor684.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'TypeError: unsupported operand type(s) for -: 'Row' and 'float''. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1019, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1009, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 3517, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 543, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 1574, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(  # type: ignore[arg-type]\n  File \"/databricks/spark/python/pyspark/statcounter.py\", line 43, in __init__\n    self.merge(v)\n  File \"/databricks/spark/python/pyspark/statcounter.py\", line 47, in merge\n    delta = value - self.mu\nTypeError: unsupported operand type(s) for -: 'Row' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:904)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1029)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:168)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:136)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 49.0 failed 1 times, most recent failure: Lost task 2.0 in stage 49.0 (TID 246) (ip-10-172-248-89.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: unsupported operand type(s) for -: 'Row' and 'float''. Full traceback below:","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-575433596223570>\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mmean\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1724\u001B[0m         \u001B[0;36m2.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1725\u001B[0m         \"\"\"\n\u001B[0;32m-> 1726\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstats\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[return-value]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1727\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1728\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mvariance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"RDD[NumberOrArray]\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;34m\"NumberOrArray\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mstats\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1572\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mleft_counter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmergeStats\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mright_counter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1574\u001B[0;31m         return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(  # type: ignore[arg-type]\n\u001B[0m\u001B[1;32m   1575\u001B[0m             \u001B[0mredFunc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1576\u001B[0m         )\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mreduce\u001B[0;34m(self, f)\u001B[0m\n\u001B[1;32m   1290\u001B[0m             \u001B[0;32myield\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minitial\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1291\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1292\u001B[0;31m         \u001B[0mvals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1293\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1294\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_local\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"logging\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m             \u001B[0;31m# no need to log since this should be internal call.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0m_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1217\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1218\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1219\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1220\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 49.0 failed 1 times, most recent failure: Lost task 2.0 in stage 49.0 (TID 246) (ip-10-172-248-89.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: unsupported operand type(s) for -: 'Row' and 'float''. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1019, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1009, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 3517, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 543, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 1574, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(  # type: ignore[arg-type]\n  File \"/databricks/spark/python/pyspark/statcounter.py\", line 43, in __init__\n    self.merge(v)\n  File \"/databricks/spark/python/pyspark/statcounter.py\", line 47, in merge\n    delta = value - self.mu\nTypeError: unsupported operand type(s) for -: 'Row' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:904)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1029)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:168)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:136)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3271)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3203)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3194)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3194)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1421)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3483)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3421)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3409)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1166)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1154)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2706)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1027)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:411)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1025)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:282)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor684.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'TypeError: unsupported operand type(s) for -: 'Row' and 'float''. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1019, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 1009, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 3517, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 543, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 1574, in <lambda>\n    return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(  # type: ignore[arg-type]\n  File \"/databricks/spark/python/pyspark/statcounter.py\", line 43, in __init__\n    self.merge(v)\n  File \"/databricks/spark/python/pyspark/statcounter.py\", line 47, in merge\n    delta = value - self.mu\nTypeError: unsupported operand type(s) for -: 'Row' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:904)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:886)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1029)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:168)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:136)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:96)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:902)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1696)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:760)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"49c53339-353b-4ad8-b134-b2605adb9841","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Pyspark Practice 1 - 15 Nov, 2022","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4267184453141756}},"nbformat":4,"nbformat_minor":0}
